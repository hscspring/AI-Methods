{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器之心 RL 学习小组测试题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What are the differences between supervised learning and reinforcement learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "维基百科给的解释是：强化学习并不需要出现正确的输入/输出对，也不需要精确校正次优化的行为。强化学习更加专注于在线规划，需要在探索（在未知的领域）和遵从（现有知识）之间找到平衡。个人认为，两者最大的不同是输入时，强化学习不需要明确的输入集（具有概念标记的训练集）；而输出时，强化学习有一个延迟奖赏或效用。监督学习通过对已有数据进行探索，着力寻找数据背后的那个抽象模型，而强化学习更加关注对结果的反馈，是一种动态的、不断选择、竞争、优胜劣汰的过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What are the essential components of an RL agent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "行为策略：决定 agent 在不同情况下的行为；奖赏函数：可看作是 agent 的目标，最大化奖赏；价值函数：从长期看来是不是最好；环境模型：包含所有可能的情形和结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Components of the Agent\n",
    "\n",
    "A reinforcement learning agent generally has 4 basic components:\n",
    "\n",
    "    a policy,\n",
    "    a reward function,\n",
    "    a value function, and\n",
    "    a model of the environment \n",
    "\n",
    "**Policy**\n",
    "\n",
    "The policy is the decision making function of the agent. It specifies what action the agent should take in any of the situations it might encounter. This is the core of the agent. The other components serve only to change and improve the policy.\n",
    "\n",
    "\n",
    "**Reward Function**\n",
    "\n",
    "The reward function defines the goal of the RL agent. It maps the state of the environment to a single number, a reward, indicating the intrinsic desirability of the state. The agent's objective is to maximize the total reward it receives in the long run.\n",
    "\n",
    "\n",
    "**Value function**\n",
    "\n",
    "The value function specifies what is good in the long run. Roughly speaking, the value of a state is the total amount of reward the agent can expect to accumulate over the future when starting from the current state.\n",
    "\n",
    "Rewards determine immediate desirability while value indicates the long term desirability.\n",
    "\n",
    "In analogy to humans, rewards are immediate pleasure (if high reward) or pain (if low) whereas values correspond to more refined far-sighted judgement of how pleased or displeased we are that our environment is in a particular state.\n",
    "\n",
    "Most of the methods we will discuss are centered around forming and improving approximate value functions.\n",
    "\n",
    "\n",
    "**Model**\n",
    "\n",
    "The model of the environment or external world should mimic the behavior of the environment. For example, given a situation and action, the model might predict the resultant next state and next reward. The model often takes up the largest storage space. If there are S states and A actions then a complete model will take up a space proportional to S x S x A because it maps state-action pairs to probability distributions over states. By contrast, the reward and value functions might just map states to real numbers and thus be of size S.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：\n",
    "- http://www.willamette.edu/~gorr/classes/cs449/Reinforcement/reinforcement1.html\n",
    "- http://www.cnblogs.com/wacc/p/5391209.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Please explain the following RL concepts:   \n",
    "\n",
    "- episodic tasks vs continuing tasks \n",
    "- tabular vs function approximation \n",
    "- batch vs online \n",
    "- on-policy vs off-policy \n",
    "- exploration vs exploitation \n",
    "- model-based vs model-free  \n",
    "- value function methods vs policy optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**episodic tasks vs continuing tasks：**agent 在与环境交互时，不断交互的称为 coutinuing task；把交互过程打散成一个个片段式任务的称为 episodic tasks。\n",
    "\n",
    "**tabular vs function approximation：**基于表格的经典算法(tabular)在小规模离散空间的强化学习任务上表现不错, 但更多的实际问题中状态数量很多, 甚至是连续状态空间, 这时经典强化学习方法难以有效学习. 特别对于连续状态空间离散化的方法, 当空间维度增加时, 离散化得到的状态数量指数增加, 因此基于表格值的强化学习算法不适用于大规模离散状态或者连续状态的强化学习任务。函数近似(Function approximation)为了克服维度灾难，将策略或者值函数用一个函数显示描述。\n",
    "\n",
    "**batch vs online：**批次学习(Batch Learning) 把所有资料都直接用于机器学习，是最常见的机器学习方式。而线上学习(Online Learning)是指对资料的运用增加一步“更新”的工作。机器学习到的东西是在一轮轮问题中更新的，每次学习一个观测。\n",
    "\n",
    "**on-policy vs off-policy：**能够从其他策略学习到最优策略的算法为离策略 (off-policy) 算法，反之为在策略 (on-policy) 算法。\n",
    "\n",
    "**exploration vs exploitation：**学习过程中，会有一个在 Exploration 和 Exploitation 之间的权衡，前者是说会放弃一些已知的 reward 信息，而去尝试一些新的选择，即在某种状态下，算法也许已经学习到选择什么 action 让 reward 比较大，但是并不能每次都做出同样的选择，也许另外一个没有尝试过的选择会让 reward 更大，即 Exploration 希望能够探索更多关于 environment 的信息。而后者是指根据已知的信息最大化 reward。\n",
    "\n",
    "**model-based vs model-free：**Model Free (模型无关)是指不需要去猜测 environment 的工作方式，而 Model based (模型相关)则是需要学习 environment 的工作方式。\n",
    "\n",
    "**value function methods vs policy optimization：**Policy Optimization 的核心思想在于，通过调整 control policy 的参数，使得 control policy 的 expected reward 最大化，目的是优化 expected reward。值函数(value function，又叫效用函数)表明当前状态下策略的长期影响，即描述 agent 以原策略进行前瞻性搜索后的长期回报。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### episodic tasks vs continuing tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agent 在与环境交互时，不断交互的称为 coutinuing task；把交互过程打散成一个个片段式任务的称为 episodic tasks。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：http://www.cnblogs.com/wacc/p/5391209.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tabular vs function approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于表格的经典算法(tabular)在小规模离散空间的强化学习任务上表现不错, 但更多的实际问题中状态数量很多, 甚至是连续状态空间, 这时经典强化学习方法难以有效学习. 特别对于连续状态空间离散化的方法, 当空间维度增加时, 离散化得到的状态数量指数增加, 因此基于表格值的强化学习算法不适用于大规模离散状态或者连续状态的强化学习任务。函数近似(Function approximation)为了克服维度灾难，将策略或者值函数用一个函数显示描述."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：http://html.rhhz.net/ZDHXBZWB/html/2016-5-685.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batch vs online"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批次学习(Batch Learning) 把所有资料都直接用于机器学习，是最常见的机器学习方式。而线上学习(Online Learning)是指对资料的运用增加一步“更新”的工作。机器学习到的东西是在一轮轮问题中更新的，每次学习一个观测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：http://iccm.cc/types-of-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### on-policy vs off-policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够从其他策略学习到最优策略的算法为离策略 (off-policy) 算法，反之为在策略 (on-policy) 算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：http://www.algorithmdog.com/reinforcement-learning-model-free-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### exploration vs exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习过程中，会有一个在 Exploration 和 Exploitation 之间的权衡，前者是说会放弃一些已知的 reward 信息，而去尝试一些新的选择，即在某种状态下，算法也许已经学习到选择什么 action 让 reward 比较大，但是并不能每次都做出同样的选择，也许另外一个没有尝试过的选择会让 reward 更大，即 Exploration 希望能够探索更多关于 environment 的信息。而后者是指根据已知的信息最大化 reward。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：\n",
    "- http://chenrudan.github.io/blog/2016/06/06/reinforcementlearninglesssion1.html\n",
    "- http://www.jiqizhixin.com/article/1883"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model-based vs model-free"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Free (模型无关)是指不需要去猜测 environment 的工作方式，而 Model based (模型相关)则是需要学习 environment 的工作方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：\n",
    "- http://chenrudan.github.io/blog/2016/06/06/reinforcementlearninglesssion1.html\n",
    "- http://www.algorithmdog.com/reinforcement-learning-model-free-evalution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### value function methods vs policy optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Optimization 的核心思想在于，通过调整 control policy 的参数，使得 control policy 的 expected reward 最大化，目的是优化 expected reward。值函数(value function，又叫效用函数)表明当前状态下策略的长期影响，即描述 agent 以原策略进行前瞻性搜索后的长期回报。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：\n",
    "- http://www.cnblogs.com/mo-wang/p/4910855.html\n",
    "- http://html.rhhz.net/ZDHXBZWB/html/2016-5-685.htm\n",
    "- https://www.zhihu.com/question/49787932?sort=created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
