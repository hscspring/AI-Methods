{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Formulation\" data-toc-modified-id=\"Formulation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Formulation</a></div><div class=\"lev2 toc-item\"><a href=\"#Action-Environment-Reward\" data-toc-modified-id=\"Action-Environment-Reward-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Action-Environment-Reward</a></div><div class=\"lev2 toc-item\"><a href=\"#实际场景\" data-toc-modified-id=\"实际场景-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>实际场景 </a></div><div class=\"lev3 toc-item\"><a href=\"#无人驾驶\" data-toc-modified-id=\"无人驾驶-121\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>无人驾驶 </a></div><div class=\"lev3 toc-item\"><a href=\"#机器人\" data-toc-modified-id=\"机器人-122\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>机器人 </a></div><div class=\"lev3 toc-item\"><a href=\"#World-of-Bits\" data-toc-modified-id=\"World-of-Bits-123\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>World of Bits</a></div><div class=\"lev3 toc-item\"><a href=\"#金融\" data-toc-modified-id=\"金融-124\"><span class=\"toc-item-num\">1.2.4&nbsp;&nbsp;</span>金融 </a></div><div class=\"lev3 toc-item\"><a href=\"#OpenAI-Gym\" data-toc-modified-id=\"OpenAI-Gym-125\"><span class=\"toc-item-num\">1.2.5&nbsp;&nbsp;</span>OpenAI Gym</a></div><div class=\"lev2 toc-item\"><a href=\"#Transition-Probability\" data-toc-modified-id=\"Transition-Probability-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Transition Probability</a></div><div class=\"lev1 toc-item\"><a href=\"#Example：捡香蕉皮机器人\" data-toc-modified-id=\"Example：捡香蕉皮机器人-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Example：捡香蕉皮机器人 </a></div><div class=\"lev1 toc-item\"><a href=\"#Episode-与-Return\" data-toc-modified-id=\"Episode-与-Return-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Episode 与 Return</a></div><div class=\"lev2 toc-item\"><a href=\"#Episode\" data-toc-modified-id=\"Episode-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Episode</a></div><div class=\"lev2 toc-item\"><a href=\"#Return\" data-toc-modified-id=\"Return-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Return</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action-Environment-Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://ohjwan9tm.bkt.clouddn.com/rl-xinyoulinxi-mdp-1.jpeg)\n",
    "\n",
    "- Agent 智能体：算法\n",
    "- Environment 环境：算法可交互\n",
    "  - 交互过程：(`S_t, R_t, A_t, S_t+1, R_t+1`)\n",
    "  - State 和 Agent 两种状态：离散，连续\n",
    "  - State 可能不存在，比如 Multi-Bandit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实际场景"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 无人驾驶\n",
    "\n",
    "- State：传感器扫描得到的点云或视频\n",
    "- Action：油门（01）、刹车（01）、方向盘（X°）\n",
    "- Reward：行驶的距离（越远越好）\n",
    "\n",
    "### 机器人\n",
    "\n",
    "- State：传感器\n",
    "- Action：不同的机器人不同\n",
    "- Reward：不同的任务不同\n",
    "\n",
    "### World of Bits\n",
    "\n",
    "- State：图片 + 解析后网页上的文本\n",
    "- Action：鼠标（点击区域）+键盘动作\n",
    "- Reward：是否完成目标（如订机票）\n",
    "\n",
    "### 金融\n",
    "\n",
    "- State：如相关股票价格\n",
    "- Action：不动、买、卖\n",
    "- Reward：收益"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Gym\n",
    "\n",
    "- CartPole: 左右移动车子平衡车顶的杆子\n",
    "- Mountain car: 把小车从底端开到顶端\n",
    "- Pendulum: 左右用力让摆锤保持竖直\n",
    "- Humanoid: 一个将近 30 维度的类人机器人\n",
    "- Atari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "for episode_i in range(20):\n",
    "    # S_t\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        # π(S_t) -> A_t \n",
    "        # 随机选择，比如向左或向右\n",
    "        action = env.action_space.sample()\n",
    "        # s_t+1, R_t\n",
    "        # done: True or False，是否已经结束\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.17392316,  0.96748011, -0.21042012, -1.64873077])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition Probability\n",
    "\n",
    "上面所有的环境都能用 Markov Decision Process 来描述。其中：\n",
    "\n",
    "- a∈A：所有 action 属于 Action space；\n",
    "- s∈S：所有 state 属于 State space；\n",
    "- r∈R：所有 reward 属于 Reward space。\n",
    "\n",
    "MDP 的核心是一个概率函数，给定前一个 state 和智能体所采取的 action，这个函数描述了后一个 state 和 reward 组合的概率分布。在随机环境中，如果把所有未来可能的 state 和 reward 组合都考虑进去，其概率之和务必是 1（概率分布的定义）。\n",
    "\n",
    "这个概率函数叫做 Transition Probability。它很精确地描述了下一步环境有可能产生的 state 或 reward，以及它们的相对不确定性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example：捡香蕉皮机器人"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- State：低电量、高电量\n",
    "- Action：\n",
    "  - 低电量：wait search recharge\n",
    "  - 高电量：wait search\n",
    "- Reward：\n",
    "  - 低电量：search，β 的概率是低电量；1-β 的概率完全失去电量抛锚（这种情况很糟糕，要尽量避免，所以给一个很大的惩罚，注意下一个状态是满电量，因为被营救回去充满电）\n",
    "  - 高电量：search，α 的概率是高电量；1-α 的概率成为低电量\n",
    "\n",
    "search 和 wait 获得的奖励一定。\n",
    "\n",
    "![](http://ohjwan9tm.bkt.clouddn.com/rl-xinyoulinxi-mdp-2.jpeg)\n",
    "\n",
    "- s: 目前的 state\n",
    "- a: action\n",
    "- s': 有可能产生的下一个 state\n",
    "- p(s'|s, a): 产生 s' 的概率\n",
    "- r(s, a, s'): 可能得到的 reward\n",
    "\n",
    "![](http://ohjwan9tm.bkt.clouddn.com/rl-xinyoulinxi-mdp-3.jpeg)\n",
    "\n",
    "其实是一种 Finite-state machine\n",
    "\n",
    "![](http://ohjwan9tm.bkt.clouddn.com/rl-xinyoulinxi-mdp-4.jpeg)\n",
    "\n",
    "有了 MDP 的 Transition Probability 之后，我们就可以回答各种各样的问题。比如，在 state s 下，做了某个 action a，那么它将平均获得多少奖励？\n",
    "\n",
    "这个未来奖励的期望值，就是函数 r(s, a) 的值。从这个表格中，我们就可估算出这个值，为：`∑_s' r(s,a,s')·p(s'|s,a)`，将所有下一步的可能情况求和。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Episode 与 Return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode\n",
    "\n",
    "如果把人生看做一场强化学习 episode，那么 action 就是每天你做的每一件决策，environment 就是这个世界，reward（奖励）就是你的幸福指数，s\\_0 初始状态是出生那一刻，s\\_T 终止状态是离开世界的那一秒。\n",
    "\n",
    "## Return \n",
    "\n",
    "未来 “一直到终止状态”，所有奖励的累计 (Reward 是下一步的奖励)\n",
    "\n",
    "- Episodic Task \n",
    "  - `Gt = R_{t+1} + R_{t+2} + ... + R_T`\n",
    "- Continuing Task \n",
    "  - 不存在终止状态\n",
    "  - `Gt = R_{t+1} + R_{t+2} + ... + R_∞`\n",
    "- Discounting\n",
    "  - `G_t = R_{t+1} + γ·R_{t+2} + γ^2·R_{t+3} ...`\n",
    "  - `G_t = Σ_{k=0→∞} r^k·R_{t+1+k}`\n",
    "  - 解决了 continuing task 不收敛的问题。\n",
    "  - 鼓励智能体选择更接近眼前的奖励，同时，也不忘考虑长远的奖励。是 Exploration 和 Exploitation 的平衡。\n",
    "- Recurrence of Return\n",
    "  - `G_t = R_{t+1} + γ·G_{t+1}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "138px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
