{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准确率\n",
    "\n",
    "分类正确的样本占总样本的比例：\n",
    "\n",
    "$$\\frac {N_{correct}} {N_{total}}$$\n",
    "\n",
    "- 问题\n",
    "\n",
    "当样本分布不均时，指标由占比大的类别决定。\n",
    "\n",
    "相对应的也有错误率：分类错误的样本占总样本的比例。\n",
    "\n",
    "写成积分的形式：\n",
    "\n",
    "$$\n",
    "error(f ; \\mathcal{D})=\\int_{\\boldsymbol{x} \\sim \\mathcal{D}} \\mathbb{I}(f(\\boldsymbol{x}) \\neq y) p(\\boldsymbol{x}) \\mathrm{d} \\boldsymbol{x}\n",
    "$$\n",
    "\n",
    "$$\n",
    "acc(f; \\mathcal{D}) = 1 - error(f ; \\mathcal{D})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "import numbers\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections.abc import Sequence\n",
    "from itertools import chain\n",
    "\n",
    "from scipy.sparse import issparse\n",
    "from scipy.sparse.base import spmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [0, 2, 1, 3]\n",
    "y_true = [0, 1, 2, 3]\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 核心\n",
    "\n",
    "其本质就是看分类器分对多少个，考察的是所有的类别。\n",
    "\n",
    "很自然地，我们就能得出这个指标受最多类别的影响最大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sklearn 源代码\n",
    "\n",
    "源代码比较复杂，考虑到了各种可能的边界和功能，主要有以下步骤：\n",
    "\n",
    "- 检查类型\n",
    "- 检查长度\n",
    "- 统计非零\n",
    "- 计算结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
    "    check_consistent_length(y_true, y_pred, sample_weight)\n",
    "    if y_type.startswith('multilabel'):\n",
    "        differing_labels = count_nonzero(y_true - y_pred, axis=1)\n",
    "        score = differing_labels == 0\n",
    "    else:\n",
    "        score = y_true == y_pred\n",
    "\n",
    "    return _weighted_sum(score, sample_weight, normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_targets(y_true, y_pred):\n",
    "    check_consistent_length(y_true, y_pred)\n",
    "    type_true = type_of_target(y_true)\n",
    "    type_pred = type_of_target(y_pred)\n",
    "    \n",
    "    # set\n",
    "    y_type = {type_true, type_pred}\n",
    "    if y_type == {\"binary\", \"multiclass\"}:\n",
    "        y_type = {\"multiclass\"}\n",
    "\n",
    "    if len(y_type) > 1:\n",
    "        raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n",
    "                         \"and {1} targets\".format(type_true, type_pred))\n",
    "\n",
    "    # We can't have more than one value on y_type => The set is no more needed\n",
    "    y_type = y_type.pop()\n",
    "\n",
    "    # No metrics support \"multiclass-multioutput\" format\n",
    "    if (y_type not in [\"binary\", \"multiclass\", \"multilabel-indicator\"]):\n",
    "        raise ValueError(\"{0} is not supported\".format(y_type))\n",
    "\n",
    "    if y_type in [\"binary\", \"multiclass\"]:\n",
    "        y_true = column_or_1d(y_true)\n",
    "        y_pred = column_or_1d(y_pred)\n",
    "        if y_type == \"binary\":\n",
    "            unique_values = np.union1d(y_true, y_pred)\n",
    "            if len(unique_values) > 2:\n",
    "                y_type = \"multiclass\"\n",
    "\n",
    "    if y_type.startswith('multilabel'):\n",
    "        y_true = csr_matrix(y_true)\n",
    "        y_pred = csr_matrix(y_pred)\n",
    "        y_type = 'multilabel-indicator'\n",
    "\n",
    "    return y_type, y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_consistent_length(*arrays):\n",
    "    \"\"\"Check that all arrays have consistent first dimensions.\n",
    "    Checks whether all objects in arrays have the same shape or length.\"\"\"\n",
    "    lengths = [_num_samples(X) for X in arrays if X is not None]\n",
    "    uniques = np.unique(lengths)\n",
    "    if len(uniques) > 1:\n",
    "        raise ValueError(\"Found input variables with inconsistent numbers of\"\n",
    "                         \" samples: %r\" % [int(l) for l in lengths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _num_samples(x):\n",
    "    \"\"\"Return number of samples in array-like x.\"\"\"\n",
    "    message = 'Expected sequence or array-like, got %s' % type(x)\n",
    "    if hasattr(x, 'fit') and callable(x.fit):\n",
    "        # Don't get num_samples from an ensembles length!\n",
    "        raise TypeError(message)\n",
    "\n",
    "    if not hasattr(x, '__len__') and not hasattr(x, 'shape'):\n",
    "        if hasattr(x, '__array__'):\n",
    "            x = np.asarray(x)\n",
    "        else:\n",
    "            raise TypeError(message)\n",
    "\n",
    "    if hasattr(x, 'shape') and x.shape is not None:\n",
    "        if len(x.shape) == 0:\n",
    "            raise TypeError(\"Singleton array %r cannot be considered\"\n",
    "                            \" a valid collection.\" % x)\n",
    "        # Check that shape is returning an integer or default to len\n",
    "        # Dask dataframes may not return numeric shape[0] value\n",
    "        if isinstance(x.shape[0], numbers.Integral):\n",
    "            return x.shape[0]\n",
    "\n",
    "    try:\n",
    "        return len(x)\n",
    "    except TypeError:\n",
    "        raise TypeError(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_of_target(y):\n",
    "    \"\"\"Determine the type of data indicated by the target.\n",
    "    Note that this type is the most specific type that can be inferred.\n",
    "    For example:\n",
    "        * ``binary`` is more specific but compatible with ``multiclass``.\n",
    "        * ``multiclass`` of integers is more specific but compatible with\n",
    "          ``continuous``.\n",
    "        * ``multilabel-indicator`` is more specific but compatible with\n",
    "          ``multiclass-multioutput``.\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array-like\n",
    "    Returns\n",
    "    -------\n",
    "    target_type : string\n",
    "        One of:\n",
    "        * 'continuous': `y` is an array-like of floats that are not all\n",
    "          integers, and is 1d or a column vector.\n",
    "        * 'continuous-multioutput': `y` is a 2d array of floats that are\n",
    "          not all integers, and both dimensions are of size > 1.\n",
    "        * 'binary': `y` contains <= 2 discrete values and is 1d or a column\n",
    "          vector.\n",
    "        * 'multiclass': `y` contains more than two discrete values, is not a\n",
    "          sequence of sequences, and is 1d or a column vector.\n",
    "        * 'multiclass-multioutput': `y` is a 2d array that contains more\n",
    "          than two discrete values, is not a sequence of sequences, and both\n",
    "          dimensions are of size > 1.\n",
    "        * 'multilabel-indicator': `y` is a label indicator matrix, an array\n",
    "          of two dimensions with at least two columns, and at most 2 unique\n",
    "          values.\n",
    "        * 'unknown': `y` is array-like but none of the above, such as a 3d\n",
    "          array, sequence of sequences, or an array of non-sequence objects.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> type_of_target([0.1, 0.6])\n",
    "    'continuous'\n",
    "    >>> type_of_target([1, -1, -1, 1])\n",
    "    'binary'\n",
    "    >>> type_of_target(['a', 'b', 'a'])\n",
    "    'binary'\n",
    "    >>> type_of_target([1.0, 2.0])\n",
    "    'binary'\n",
    "    >>> type_of_target([1, 0, 2])\n",
    "    'multiclass'\n",
    "    >>> type_of_target([1.0, 0.0, 3.0])\n",
    "    'multiclass'\n",
    "    >>> type_of_target(['a', 'b', 'c'])\n",
    "    'multiclass'\n",
    "    >>> type_of_target(np.array([[1, 2], [3, 1]]))\n",
    "    'multiclass-multioutput'\n",
    "    >>> type_of_target([[1, 2]])\n",
    "    'multilabel-indicator'\n",
    "    >>> type_of_target(np.array([[1.5, 2.0], [3.0, 1.6]]))\n",
    "    'continuous-multioutput'\n",
    "    >>> type_of_target(np.array([[0, 1], [1, 1]]))\n",
    "    'multilabel-indicator'\n",
    "    \"\"\"\n",
    "    valid = ((isinstance(y, (Sequence, spmatrix)) or hasattr(y, '__array__'))\n",
    "             and not isinstance(y, str))\n",
    "\n",
    "    if not valid:\n",
    "        raise ValueError('Expected array-like (array or non-string sequence), '\n",
    "                         'got %r' % y)\n",
    "\n",
    "    sparse_pandas = (y.__class__.__name__ in ['SparseSeries', 'SparseArray'])\n",
    "    if sparse_pandas:\n",
    "        raise ValueError(\"y cannot be class 'SparseSeries' or 'SparseArray'\")\n",
    "\n",
    "    if is_multilabel(y):\n",
    "        return 'multilabel-indicator'\n",
    "\n",
    "    try:\n",
    "        y = np.asarray(y)\n",
    "    except ValueError:\n",
    "        # Known to fail in numpy 1.3 for array of arrays\n",
    "        return 'unknown'\n",
    "\n",
    "    # The old sequence of sequences format\n",
    "    try:\n",
    "        if (not hasattr(y[0], '__array__') and isinstance(y[0], Sequence)\n",
    "                and not isinstance(y[0], str)):\n",
    "            raise ValueError('You appear to be using a legacy multi-label data'\n",
    "                             ' representation. Sequence of sequences are no'\n",
    "                             ' longer supported; use a binary array or sparse'\n",
    "                             ' matrix instead - the MultiLabelBinarizer'\n",
    "                             ' transformer can convert to this format.')\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    # Invalid inputs\n",
    "    if y.ndim > 2 or (y.dtype == object and len(y) and\n",
    "                      not isinstance(y.flat[0], str)):\n",
    "        return 'unknown'  # [[[1, 2]]] or [obj_1] and not [\"label_1\"]\n",
    "\n",
    "    if y.ndim == 2 and y.shape[1] == 0:\n",
    "        return 'unknown'  # [[]]\n",
    "\n",
    "    if y.ndim == 2 and y.shape[1] > 1:\n",
    "        suffix = \"-multioutput\"  # [[1, 2], [1, 2]]\n",
    "    else:\n",
    "        suffix = \"\"  # [1, 2, 3] or [[1], [2], [3]]\n",
    "\n",
    "    # check float and contains non-integer float values\n",
    "    if y.dtype.kind == 'f' and np.any(y != y.astype(int)):\n",
    "        # [.1, .2, 3] or [[.1, .2, 3]] or [[1., .2]] and not [1., 2., 3.]\n",
    "        _assert_all_finite(y)\n",
    "        return 'continuous' + suffix\n",
    "\n",
    "    if (len(np.unique(y)) > 2) or (y.ndim >= 2 and len(y[0]) > 1):\n",
    "        return 'multiclass' + suffix  # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]\n",
    "    else:\n",
    "        return 'binary'  # [1, 2] or [[\"a\"], [\"b\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_multilabel(y):\n",
    "    \"\"\" Check if ``y`` is in a multilabel format.\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : numpy array of shape [n_samples]\n",
    "        Target values.\n",
    "    Returns\n",
    "    -------\n",
    "    out : bool,\n",
    "        Return ``True``, if ``y`` is in a multilabel format, else ```False``.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> from sklearn.utils.multiclass import is_multilabel\n",
    "    >>> is_multilabel([0, 1, 0, 1])\n",
    "    False\n",
    "    >>> is_multilabel([[1], [0, 2], []])\n",
    "    False\n",
    "    >>> is_multilabel(np.array([[1, 0], [0, 0]]))\n",
    "    True\n",
    "    >>> is_multilabel(np.array([[1], [0], [0]]))\n",
    "    False\n",
    "    >>> is_multilabel(np.array([[1, 0, 0]]))\n",
    "    True\n",
    "    \"\"\"\n",
    "    if hasattr(y, '__array__') or isinstance(y, Sequence):\n",
    "        y = np.asarray(y)\n",
    "    if not (hasattr(y, \"shape\") and y.ndim == 2 and y.shape[1] > 1):\n",
    "        return False\n",
    "\n",
    "    if issparse(y):\n",
    "        if isinstance(y, (dok_matrix, lil_matrix)):\n",
    "            y = y.tocsr()\n",
    "        return (len(y.data) == 0 or np.unique(y.data).size == 1 and\n",
    "                (y.dtype.kind in 'biu' or  # bool, int, uint\n",
    "                 _is_integral_float(np.unique(y.data))))\n",
    "    else:\n",
    "        labels = np.unique(y)\n",
    "\n",
    "        return len(labels) < 3 and (y.dtype.kind in 'biu' or  # bool, int, uint\n",
    "                                    _is_integral_float(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_or_1d(y, warn=False):\n",
    "    \"\"\" Ravel column or 1d numpy array, else raises an error\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array-like\n",
    "    warn : boolean, default False\n",
    "       To control display of warnings.\n",
    "    Returns\n",
    "    -------\n",
    "    y : array\n",
    "    \"\"\"\n",
    "    y = np.asarray(y)\n",
    "    shape = np.shape(y)\n",
    "    if len(shape) == 1:\n",
    "        return np.ravel(y)\n",
    "    if len(shape) == 2 and shape[1] == 1:\n",
    "        if warn:\n",
    "            warnings.warn(\"A column-vector y was passed when a 1d array was\"\n",
    "                          \" expected. Please change the shape of y to \"\n",
    "                          \"(n_samples, ), for example using ravel().\",\n",
    "                          DataConversionWarning, stacklevel=2)\n",
    "        return np.ravel(y)\n",
    "\n",
    "    raise ValueError(\"bad input shape {0}\".format(shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nonzero(X, axis=None, sample_weight=None):\n",
    "    \"\"\"A variant of X.getnnz() with extension to weighting on axis 0\n",
    "    Useful in efficiently calculating multilabel metrics.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : CSR sparse matrix of shape (n_samples, n_labels)\n",
    "        Input data.\n",
    "    axis : None, 0 or 1\n",
    "        The axis on which the data is aggregated.\n",
    "    sample_weight : array-like of shape (n_samples,), default=None\n",
    "        Weight for each row of X.\n",
    "    \"\"\"\n",
    "    if axis == -1:\n",
    "        axis = 1\n",
    "    elif axis == -2:\n",
    "        axis = 0\n",
    "    elif X.format != 'csr':\n",
    "        raise TypeError('Expected CSR sparse format, got {0}'.format(X.format))\n",
    "\n",
    "    # We rely here on the fact that np.diff(Y.indptr) for a CSR\n",
    "    # will return the number of nonzero entries in each row.\n",
    "    # A bincount over Y.indices will return the number of nonzeros\n",
    "    # in each column. See ``csr_matrix.getnnz`` in scipy >= 0.14.\n",
    "    if axis is None:\n",
    "        if sample_weight is None:\n",
    "            return X.nnz\n",
    "        else:\n",
    "            return np.dot(np.diff(X.indptr), sample_weight)\n",
    "    elif axis == 1:\n",
    "        out = np.diff(X.indptr)\n",
    "        if sample_weight is None:\n",
    "            # astype here is for consistency with axis=0 dtype\n",
    "            return out.astype('intp')\n",
    "        return out * sample_weight\n",
    "    elif axis == 0:\n",
    "        if sample_weight is None:\n",
    "            return np.bincount(X.indices, minlength=X.shape[1])\n",
    "        else:\n",
    "            weights = np.repeat(sample_weight, np.diff(X.indptr))\n",
    "            return np.bincount(X.indices, minlength=X.shape[1],\n",
    "                            weights=weights)\n",
    "    else:\n",
    "        raise ValueError('Unsupported axis: {0}'.format(axis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _weighted_sum(sample_score, sample_weight, normalize=False):\n",
    "    if normalize:\n",
    "        return np.average(sample_score, weights=sample_weight)\n",
    "    elif sample_weight is not None:\n",
    "        return np.dot(sample_score, sample_weight)\n",
    "    else:\n",
    "        return sample_score.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P-R 与 F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P（precision）是精准率，也叫查准率；\n",
    "\n",
    "R（recall）是召回率，也叫查全率。\n",
    "\n",
    "要想理解二者的含义，最好先知道混淆矩阵：\n",
    "\n",
    "真实情况 | 预测结果正例| 预测结果反例\n",
    "----|----|----\n",
    "正例 | TP（真正例）|FN（假反例）\n",
    "反例 | FP（假正例）|TN（真反例）\n",
    "\n",
    "$$P = \\frac {TP}{TP+FP}$$\n",
    "\n",
    "$$R = \\frac {TP}{TP+FN}$$\n",
    "\n",
    "以前老是记不住，后来也就不记了，用的时候把混淆矩阵拿出来一看就知道了。通俗点解释就是：精准率是模型判断是正例中有多少是真正的正例；召回率是模型判断是真正的正例与实际正例的比例。\n",
    "\n",
    "PR 可以绘制 P-R 曲线图，纵坐标是 P 横坐标是 R，一般而言二者是互相对立的，也就是说高 P 一般对应低 R，反之亦然。\n",
    "\n",
    "所以，这之间必然有一个平衡点，P-R 曲线的平衡点叫 BEP（Break-Even Point），平衡点越靠右上表示模型越好。\n",
    "\n",
    "在实际中，更常用的是 F1 度量：\n",
    "\n",
    "$$\n",
    "F1 = \\frac {2PR}{P+R}\n",
    "$$\n",
    "\n",
    "F1 有更一般的形式：\n",
    "\n",
    "$$\n",
    "F_{\\beta}=\\frac{\\left(1+\\beta^{2}\\right) \\times P \\times R}{\\left(\\beta^{2} \\times P\\right)+R}\n",
    "$$\n",
    "\n",
    "详细可参考：[Naive Bayes and Sentiment Classification Note (SLP Ch04) | Yam](https://yam.gift/2019/05/05/SLP/2019-05-05-NaiveBayes-and-Sentiment-Classification/)\n",
    "\n",
    "另外，多次评估时，有两种方式考察 PR：\n",
    "\n",
    "- 先计算每个 PR，取平均后，再计算 F1：macro 方法\n",
    "- 先计算混淆矩阵元素的平均，再计算 PR 和 F1：micro 方法\n",
    "\n",
    "记忆的时候可以这么理解：micro 方法只要计算一次 PR，而 macro 方法需要计算多次 PR。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 核心"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P-R 和 F1 本质上是基于正例的，准确来说是模型预测的真正例。\n",
    "\n",
    "P 是精准率，也就是预测出来的所有正例中，是真正例的比例；\n",
    "\n",
    "R 是召回率，也就是所有实际真正正例中，预测为真正例的比例。\n",
    "\n",
    "前者关注的是预测的准确性，后者关注的是有多少结果被预测出来了。\n",
    "\n",
    "这样的评估方式比起 Accuracy 就进步太多了，尤其对于不同类别分布不均匀的情况。此时精准率可能很高，但召回率却很低。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sklearn 源代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "源代码主要有以下步骤：\n",
    "\n",
    "- 检查计算中有没有 0 值，比如没有 positive 的 Recall\n",
    "- 检查 label\n",
    "- 计算混淆矩阵\n",
    "- 计算 p r\n",
    "\n",
    "\n",
    "我已经被下面这波操作震到了，看来对 “工程” 理解还是很浅薄啊，这才是需要学习的代码。先过完一遍回头来补。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from inspect import signature, isclass, Parameter\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
    "y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6666666666666666, 0.6666666666666666, 0.6666666666666666, None)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(y_true, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UndefinedMetricWarning(UserWarning):\n",
    "    \"\"\"Warning used when the metric is invalid\n",
    "    .. versionchanged:: 0.18\n",
    "       Moved from sklearn.base.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision_score, recall_score, f1_score 都是基于本函数\n",
    "def precision_recall_fscore_support(y_true, y_pred, beta=1.0, labels=None,\n",
    "                                    pos_label=1, average=None,\n",
    "                                    warn_for=('precision', 'recall',\n",
    "                                              'f-score'),\n",
    "                                    sample_weight=None,\n",
    "                                    zero_division=\"warn\"):\n",
    "    \"\"\"Compute precision, recall, F-measure and support for each class\n",
    "    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
    "    true positives and ``fp`` the number of false positives. The precision is\n",
    "    intuitively the ability of the classifier not to label as positive a sample\n",
    "    that is negative.\n",
    "    The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
    "    true positives and ``fn`` the number of false negatives. The recall is\n",
    "    intuitively the ability of the classifier to find all the positive samples.\n",
    "    The F-beta score can be interpreted as a weighted harmonic mean of\n",
    "    the precision and recall, where an F-beta score reaches its best\n",
    "    value at 1 and worst score at 0.\n",
    "    The F-beta score weights recall more than precision by a factor of\n",
    "    ``beta``. ``beta == 1.0`` means recall and precision are equally important.\n",
    "    The support is the number of occurrences of each class in ``y_true``.\n",
    "    If ``pos_label is None`` and in binary classification, this function\n",
    "    returns the average precision, recall and F-measure if ``average``\n",
    "    is one of ``'micro'``, ``'macro'``, ``'weighted'`` or ``'samples'``.\n",
    "    Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : 1d array-like, or label indicator array / sparse matrix\n",
    "        Ground truth (correct) target values.\n",
    "    y_pred : 1d array-like, or label indicator array / sparse matrix\n",
    "        Estimated targets as returned by a classifier.\n",
    "    beta : float, 1.0 by default\n",
    "        The strength of recall versus precision in the F-score.\n",
    "    labels : list, optional\n",
    "        The set of labels to include when ``average != 'binary'``, and their\n",
    "        order if ``average is None``. Labels present in the data can be\n",
    "        excluded, for example to calculate a multiclass average ignoring a\n",
    "        majority negative class, while labels not present in the data will\n",
    "        result in 0 components in a macro average. For multilabel targets,\n",
    "        labels are column indices. By default, all labels in ``y_true`` and\n",
    "        ``y_pred`` are used in sorted order.\n",
    "    pos_label : str or int, 1 by default\n",
    "        The class to report if ``average='binary'`` and the data is binary.\n",
    "        If the data are multiclass or multilabel, this will be ignored;\n",
    "        setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
    "        scores for that label only.\n",
    "    average : string, [None (default), 'binary', 'micro', 'macro', 'samples', \\\n",
    "                       'weighted']\n",
    "        If ``None``, the scores for each class are returned. Otherwise, this\n",
    "        determines the type of averaging performed on the data:\n",
    "        ``'binary'``:\n",
    "            Only report results for the class specified by ``pos_label``.\n",
    "            This is applicable only if targets (``y_{true,pred}``) are binary.\n",
    "        ``'micro'``:\n",
    "            Calculate metrics globally by counting the total true positives,\n",
    "            false negatives and false positives.\n",
    "        ``'macro'``:\n",
    "            Calculate metrics for each label, and find their unweighted\n",
    "            mean.  This does not take label imbalance into account.\n",
    "        ``'weighted'``:\n",
    "            Calculate metrics for each label, and find their average weighted\n",
    "            by support (the number of true instances for each label). This\n",
    "            alters 'macro' to account for label imbalance; it can result in an\n",
    "            F-score that is not between precision and recall.\n",
    "        ``'samples'``:\n",
    "            Calculate metrics for each instance, and find their average (only\n",
    "            meaningful for multilabel classification where this differs from\n",
    "            :func:`accuracy_score`).\n",
    "    warn_for : tuple or set, for internal use\n",
    "        This determines which warnings will be made in the case that this\n",
    "        function is being used to return only one of its metrics.\n",
    "    sample_weight : array-like of shape (n_samples,), default=None\n",
    "        Sample weights.\n",
    "    zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
    "        Sets the value to return when there is a zero division:\n",
    "           - recall: when there are no positive labels\n",
    "           - precision: when there are no positive predictions\n",
    "           - f-score: both\n",
    "        If set to \"warn\", this acts as 0, but warnings are also raised.\n",
    "    Returns\n",
    "    -------\n",
    "    precision : float (if average is not None) or array of float, shape =\\\n",
    "        [n_unique_labels]\n",
    "    recall : float (if average is not None) or array of float, , shape =\\\n",
    "        [n_unique_labels]\n",
    "    fbeta_score : float (if average is not None) or array of float, shape =\\\n",
    "        [n_unique_labels]\n",
    "    support : int (if average is not None) or array of int, shape =\\\n",
    "        [n_unique_labels]\n",
    "        The number of occurrences of each label in ``y_true``.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] `Wikipedia entry for the Precision and recall\n",
    "           <https://en.wikipedia.org/wiki/Precision_and_recall>`_\n",
    "    .. [2] `Wikipedia entry for the F1-score\n",
    "           <https://en.wikipedia.org/wiki/F1_score>`_\n",
    "    .. [3] `Discriminative Methods for Multi-labeled Classification Advances\n",
    "           in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu\n",
    "           Godbole, Sunita Sarawagi\n",
    "           <http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf>`_\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> from sklearn.metrics import precision_recall_fscore_support\n",
    "    >>> y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig'])\n",
    "    >>> y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog'])\n",
    "    >>> precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "    (0.22..., 0.33..., 0.26..., None)\n",
    "    >>> precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "    (0.33..., 0.33..., 0.33..., None)\n",
    "    >>> precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    (0.22..., 0.33..., 0.26..., None)\n",
    "    It is possible to compute per-label precisions, recalls, F1-scores and\n",
    "    supports instead of averaging:\n",
    "    >>> precision_recall_fscore_support(y_true, y_pred, average=None,\n",
    "    ... labels=['pig', 'dog', 'cat'])\n",
    "    (array([0.        , 0.        , 0.66...]),\n",
    "     array([0., 0., 1.]), array([0. , 0. , 0.8]),\n",
    "     array([2, 2, 2]))\n",
    "    Notes\n",
    "    -----\n",
    "    When ``true positive + false positive == 0``, precision is undefined;\n",
    "    When ``true positive + false negative == 0``, recall is undefined.\n",
    "    In such cases, by default the metric will be set to 0, as will f-score,\n",
    "    and ``UndefinedMetricWarning`` will be raised. This behavior can be\n",
    "    modified with ``zero_division``.\n",
    "    \"\"\"\n",
    "    _check_zero_division(zero_division)\n",
    "    if beta < 0:\n",
    "        raise ValueError(\"beta should be >=0 in the F-beta score\")\n",
    "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
    "                                    pos_label)\n",
    "\n",
    "    # Calculate tp_sum, pred_sum, true_sum ###\n",
    "    samplewise = average == 'samples'\n",
    "    MCM = multilabel_confusion_matrix(y_true, y_pred,\n",
    "                                      sample_weight=sample_weight,\n",
    "                                      labels=labels, samplewise=samplewise)\n",
    "    tp_sum = MCM[:, 1, 1]\n",
    "    pred_sum = tp_sum + MCM[:, 0, 1]\n",
    "    true_sum = tp_sum + MCM[:, 1, 0]\n",
    "\n",
    "    if average == 'micro':\n",
    "        tp_sum = np.array([tp_sum.sum()])\n",
    "        pred_sum = np.array([pred_sum.sum()])\n",
    "        true_sum = np.array([true_sum.sum()])\n",
    "\n",
    "    # Finally, we have all our sufficient statistics. Divide! #\n",
    "    beta2 = beta ** 2\n",
    "\n",
    "    # Divide, and on zero-division, set scores and/or warn according to\n",
    "    # zero_division:\n",
    "    precision = _prf_divide(tp_sum, pred_sum, 'precision',\n",
    "                            'predicted', average, warn_for, zero_division)\n",
    "    recall = _prf_divide(tp_sum, true_sum, 'recall',\n",
    "                         'true', average, warn_for, zero_division)\n",
    "\n",
    "    # warn for f-score only if zero_division is warn, it is in warn_for\n",
    "    # and BOTH prec and rec are ill-defined\n",
    "    if zero_division == \"warn\" and (\"f-score\",) == warn_for:\n",
    "        if (pred_sum[true_sum == 0] == 0).any():\n",
    "            _warn_prf(\n",
    "                average, \"true nor predicted\", 'F-score is', len(true_sum)\n",
    "            )\n",
    "\n",
    "    # if tp == 0 F will be 1 only if all predictions are zero, all labels are\n",
    "    # zero, and zero_division=1. In all other case, 0\n",
    "    if np.isposinf(beta):\n",
    "        f_score = recall\n",
    "    else:\n",
    "        denom = beta2 * precision + recall\n",
    "\n",
    "        denom[denom == 0.] = 1  # avoid division by 0\n",
    "        f_score = (1 + beta2) * precision * recall / denom\n",
    "\n",
    "    # Average the results\n",
    "    if average == 'weighted':\n",
    "        weights = true_sum\n",
    "        if weights.sum() == 0:\n",
    "            zero_division_value = 0.0 if zero_division in [\"warn\", 0] else 1.0\n",
    "            # precision is zero_division if there are no positive predictions\n",
    "            # recall is zero_division if there are no positive labels\n",
    "            # fscore is zero_division if all labels AND predictions are\n",
    "            # negative\n",
    "            return (zero_division_value if pred_sum.sum() == 0 else 0,\n",
    "                    zero_division_value,\n",
    "                    zero_division_value if pred_sum.sum() == 0 else 0,\n",
    "                    None)\n",
    "\n",
    "    elif average == 'samples':\n",
    "        weights = sample_weight\n",
    "    else:\n",
    "        weights = None\n",
    "\n",
    "    if average is not None:\n",
    "        assert average != 'binary' or len(precision) == 1\n",
    "        precision = np.average(precision, weights=weights)\n",
    "        recall = np.average(recall, weights=weights)\n",
    "        f_score = np.average(f_score, weights=weights)\n",
    "        true_sum = None  # return no support\n",
    "\n",
    "    return precision, recall, f_score, true_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _warn_prf(average, modifier, msg_start, result_size):\n",
    "    axis0, axis1 = 'sample', 'label'\n",
    "    if average == 'samples':\n",
    "        axis0, axis1 = axis1, axis0\n",
    "    msg = ('{0} ill-defined and being set to 0.0 {{0}} '\n",
    "           'no {1} {2}s. Use `zero_division` parameter to control'\n",
    "           ' this behavior.'.format(msg_start, modifier, axis0))\n",
    "    if result_size == 1:\n",
    "        msg = msg.format('due to')\n",
    "    else:\n",
    "        msg = msg.format('in {0}s with'.format(axis1))\n",
    "    warnings.warn(msg, UndefinedMetricWarning, stacklevel=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prf_divide(numerator, denominator, metric,\n",
    "                modifier, average, warn_for, zero_division=\"warn\"):\n",
    "    \"\"\"Performs division and handles divide-by-zero.\n",
    "    On zero-division, sets the corresponding result elements equal to\n",
    "    0 or 1 (according to ``zero_division``). Plus, if\n",
    "    ``zero_division != \"warn\"`` raises a warning.\n",
    "    The metric, modifier and average arguments are used only for determining\n",
    "    an appropriate warning.\n",
    "    \"\"\"\n",
    "    mask = denominator == 0.0\n",
    "    denominator = denominator.copy()\n",
    "    denominator[mask] = 1  # avoid infs/nans\n",
    "    result = numerator / denominator\n",
    "\n",
    "    if not np.any(mask):\n",
    "        return result\n",
    "\n",
    "    # if ``zero_division=1``, set those with denominator == 0 equal to 1\n",
    "    result[mask] = 0.0 if zero_division in [\"warn\", 0] else 1.0\n",
    "\n",
    "    # the user will be removing warnings if zero_division is set to something\n",
    "    # different than its default value. If we are computing only f-score\n",
    "    # the warning will be raised only if precision and recall are ill-defined\n",
    "    if zero_division != \"warn\" or metric not in warn_for:\n",
    "        return result\n",
    "\n",
    "    # build appropriate warning\n",
    "    # E.g. \"Precision and F-score are ill-defined and being set to 0.0 in\n",
    "    # labels with no predicted samples. Use ``zero_division`` parameter to\n",
    "    # control this behavior.\"\n",
    "\n",
    "    if metric in warn_for and 'f-score' in warn_for:\n",
    "        msg_start = '{0} and F-score are'.format(metric.title())\n",
    "    elif metric in warn_for:\n",
    "        msg_start = '{0} is'.format(metric.title())\n",
    "    elif 'f-score' in warn_for:\n",
    "        msg_start = 'F-score is'\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "    _warn_prf(average, modifier, msg_start, len(result))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_zero_division(zero_division):\n",
    "    if isinstance(zero_division, str) and zero_division == \"warn\":\n",
    "        return\n",
    "    elif isinstance(zero_division, (int, float)) and zero_division in [0, 1]:\n",
    "        return\n",
    "    raise ValueError('Got zero_division={0}.'\n",
    "                     ' Must be one of [\"warn\", 0, 1]'.format(zero_division))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_set_wise_labels(y_true, y_pred, average, labels, pos_label):\n",
    "    \"\"\"Validation associated with set-wise metrics\n",
    "    Returns identified labels\n",
    "    \"\"\"\n",
    "    average_options = (None, 'micro', 'macro', 'weighted', 'samples')\n",
    "    if average not in average_options and average != 'binary':\n",
    "        raise ValueError('average has to be one of ' +\n",
    "                         str(average_options))\n",
    "\n",
    "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
    "    present_labels = unique_labels(y_true, y_pred)\n",
    "    if average == 'binary':\n",
    "        if y_type == 'binary':\n",
    "            if pos_label not in present_labels:\n",
    "                if len(present_labels) >= 2:\n",
    "                    raise ValueError(\"pos_label=%r is not a valid label: \"\n",
    "                                     \"%r\" % (pos_label, present_labels))\n",
    "            labels = [pos_label]\n",
    "        else:\n",
    "            average_options = list(average_options)\n",
    "            if y_type == 'multiclass':\n",
    "                average_options.remove('samples')\n",
    "            raise ValueError(\"Target is %s but average='binary'. Please \"\n",
    "                             \"choose another average setting, one of %r.\"\n",
    "                             % (y_type, average_options))\n",
    "    elif pos_label not in (None, 1):\n",
    "        warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
    "                      \"average != 'binary' (got %r). You may use \"\n",
    "                      \"labels=[pos_label] to specify a single positive class.\"\n",
    "                      % (pos_label, average), UserWarning)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_labels(*ys):\n",
    "    \"\"\"Extract an ordered array of unique labels\n",
    "    We don't allow:\n",
    "        - mix of multilabel and multiclass (single label) targets\n",
    "        - mix of label indicator matrix and anything else,\n",
    "          because there are no explicit labels)\n",
    "        - mix of label indicator matrices of different sizes\n",
    "        - mix of string and integer labels\n",
    "    At the moment, we also don't allow \"multiclass-multioutput\" input type.\n",
    "    Parameters\n",
    "    ----------\n",
    "    *ys : array-likes\n",
    "    Returns\n",
    "    -------\n",
    "    out : numpy array of shape [n_unique_labels]\n",
    "        An ordered array of unique labels.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.utils.multiclass import unique_labels\n",
    "    >>> unique_labels([3, 5, 5, 5, 7, 7])\n",
    "    array([3, 5, 7])\n",
    "    >>> unique_labels([1, 2, 3, 4], [2, 2, 3, 4])\n",
    "    array([1, 2, 3, 4])\n",
    "    >>> unique_labels([1, 2, 10], [5, 11])\n",
    "    array([ 1,  2,  5, 10, 11])\n",
    "    \"\"\"\n",
    "    if not ys:\n",
    "        raise ValueError('No argument has been passed.')\n",
    "    # Check that we don't mix label format\n",
    "\n",
    "    ys_types = set(type_of_target(x) for x in ys)\n",
    "    if ys_types == {\"binary\", \"multiclass\"}:\n",
    "        ys_types = {\"multiclass\"}\n",
    "\n",
    "    if len(ys_types) > 1:\n",
    "        raise ValueError(\"Mix type of y not allowed, got types %s\" % ys_types)\n",
    "\n",
    "    label_type = ys_types.pop()\n",
    "\n",
    "    # Check consistency for the indicator format\n",
    "    if (label_type == \"multilabel-indicator\" and\n",
    "            len(set(check_array(y, ['csr', 'csc', 'coo']).shape[1]\n",
    "                    for y in ys)) > 1):\n",
    "        raise ValueError(\"Multi-label binary indicator input with \"\n",
    "                         \"different numbers of labels\")\n",
    "\n",
    "    # Get the unique set of labels\n",
    "    _unique_labels = _FN_UNIQUE_LABELS.get(label_type, None)\n",
    "    if not _unique_labels:\n",
    "        raise ValueError(\"Unknown label type: %s\" % repr(ys))\n",
    "\n",
    "    ys_labels = set(chain.from_iterable(_unique_labels(y) for y in ys))\n",
    "\n",
    "    # Check that we don't mix string type with number type\n",
    "    if (len(set(isinstance(label, str) for label in ys_labels)) > 1):\n",
    "        raise ValueError(\"Mix of label input types (string and number)\")\n",
    "\n",
    "    return np.array(sorted(ys_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _unique_multiclass(y):\n",
    "    if hasattr(y, '__array__'):\n",
    "        return np.unique(np.asarray(y))\n",
    "    else:\n",
    "        return set(y)\n",
    "\n",
    "\n",
    "def _unique_indicator(y):\n",
    "    return np.arange(check_array(y, ['csr', 'csc', 'coo']).shape[1])\n",
    "\n",
    "\n",
    "_FN_UNIQUE_LABELS = {\n",
    "    'binary': _unique_multiclass,\n",
    "    'multiclass': _unique_multiclass,\n",
    "    'multilabel-indicator': _unique_indicator,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilabel_confusion_matrix(y_true, y_pred, sample_weight=None,\n",
    "                                labels=None, samplewise=False):\n",
    "    \"\"\"Compute a confusion matrix for each class or sample\n",
    "    .. versionadded:: 0.21\n",
    "    Compute class-wise (default) or sample-wise (samplewise=True) multilabel\n",
    "    confusion matrix to evaluate the accuracy of a classification, and output\n",
    "    confusion matrices for each class or sample.\n",
    "    In multilabel confusion matrix :math:`MCM`, the count of true negatives\n",
    "    is :math:`MCM_{:,0,0}`, false negatives is :math:`MCM_{:,1,0}`,\n",
    "    true positives is :math:`MCM_{:,1,1}` and false positives is\n",
    "    :math:`MCM_{:,0,1}`.\n",
    "    Multiclass data will be treated as if binarized under a one-vs-rest\n",
    "    transformation. Returned confusion matrices will be in the order of\n",
    "    sorted unique labels in the union of (y_true, y_pred).\n",
    "    Read more in the :ref:`User Guide <multilabel_confusion_matrix>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : 1d array-like, or label indicator array / sparse matrix\n",
    "        of shape (n_samples, n_outputs) or (n_samples,)\n",
    "        Ground truth (correct) target values.\n",
    "    y_pred : 1d array-like, or label indicator array / sparse matrix\n",
    "        of shape (n_samples, n_outputs) or (n_samples,)\n",
    "        Estimated targets as returned by a classifier\n",
    "    sample_weight : array-like of shape (n_samples,), default=None\n",
    "        Sample weights\n",
    "    labels : array-like\n",
    "        A list of classes or column indices to select some (or to force\n",
    "        inclusion of classes absent from the data)\n",
    "    samplewise : bool, default=False\n",
    "        In the multilabel case, this calculates a confusion matrix per sample\n",
    "    Returns\n",
    "    -------\n",
    "    multi_confusion : array, shape (n_outputs, 2, 2)\n",
    "        A 2x2 confusion matrix corresponding to each output in the input.\n",
    "        When calculating class-wise multi_confusion (default), then\n",
    "        n_outputs = n_labels; when calculating sample-wise multi_confusion\n",
    "        (samplewise=True), n_outputs = n_samples. If ``labels`` is defined,\n",
    "        the results will be returned in the order specified in ``labels``,\n",
    "        otherwise the results will be returned in sorted order by default.\n",
    "    See also\n",
    "    --------\n",
    "    confusion_matrix\n",
    "    Notes\n",
    "    -----\n",
    "    The multilabel_confusion_matrix calculates class-wise or sample-wise\n",
    "    multilabel confusion matrices, and in multiclass tasks, labels are\n",
    "    binarized under a one-vs-rest way; while confusion_matrix calculates\n",
    "    one confusion matrix for confusion between every two classes.\n",
    "    Examples\n",
    "    --------\n",
    "    Multilabel-indicator case:\n",
    "    >>> import numpy as np\n",
    "    >>> from sklearn.metrics import multilabel_confusion_matrix\n",
    "    >>> y_true = np.array([[1, 0, 1],\n",
    "    ...                    [0, 1, 0]])\n",
    "    >>> y_pred = np.array([[1, 0, 0],\n",
    "    ...                    [0, 1, 1]])\n",
    "    >>> multilabel_confusion_matrix(y_true, y_pred)\n",
    "    array([[[1, 0],\n",
    "            [0, 1]],\n",
    "    <BLANKLINE>\n",
    "           [[1, 0],\n",
    "            [0, 1]],\n",
    "    <BLANKLINE>\n",
    "           [[0, 1],\n",
    "            [1, 0]]])\n",
    "    Multiclass case:\n",
    "    >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
    "    >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
    "    >>> multilabel_confusion_matrix(y_true, y_pred,\n",
    "    ...                             labels=[\"ant\", \"bird\", \"cat\"])\n",
    "    array([[[3, 1],\n",
    "            [0, 2]],\n",
    "    <BLANKLINE>\n",
    "           [[5, 0],\n",
    "            [1, 0]],\n",
    "    <BLANKLINE>\n",
    "           [[2, 1],\n",
    "            [1, 2]]])\n",
    "    \"\"\"\n",
    "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
    "    if sample_weight is not None:\n",
    "        sample_weight = column_or_1d(sample_weight)\n",
    "    check_consistent_length(y_true, y_pred, sample_weight)\n",
    "\n",
    "    if y_type not in (\"binary\", \"multiclass\", \"multilabel-indicator\"):\n",
    "        raise ValueError(\"%s is not supported\" % y_type)\n",
    "\n",
    "    present_labels = unique_labels(y_true, y_pred)\n",
    "    if labels is None:\n",
    "        labels = present_labels\n",
    "        n_labels = None\n",
    "    else:\n",
    "        n_labels = len(labels)\n",
    "        labels = np.hstack([labels, np.setdiff1d(present_labels, labels,\n",
    "                                                 assume_unique=True)])\n",
    "\n",
    "    if y_true.ndim == 1:\n",
    "        if samplewise:\n",
    "            raise ValueError(\"Samplewise metrics are not available outside of \"\n",
    "                             \"multilabel classification.\")\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        le.fit(labels)\n",
    "        y_true = le.transform(y_true)\n",
    "        y_pred = le.transform(y_pred)\n",
    "        sorted_labels = le.classes_\n",
    "\n",
    "        # labels are now from 0 to len(labels) - 1 -> use bincount\n",
    "        tp = y_true == y_pred\n",
    "        tp_bins = y_true[tp]\n",
    "        if sample_weight is not None:\n",
    "            tp_bins_weights = np.asarray(sample_weight)[tp]\n",
    "        else:\n",
    "            tp_bins_weights = None\n",
    "\n",
    "        if len(tp_bins):\n",
    "            tp_sum = np.bincount(tp_bins, weights=tp_bins_weights,\n",
    "                                 minlength=len(labels))\n",
    "        else:\n",
    "            # Pathological case\n",
    "            true_sum = pred_sum = tp_sum = np.zeros(len(labels))\n",
    "        if len(y_pred):\n",
    "            pred_sum = np.bincount(y_pred, weights=sample_weight,\n",
    "                                   minlength=len(labels))\n",
    "        if len(y_true):\n",
    "            true_sum = np.bincount(y_true, weights=sample_weight,\n",
    "                                   minlength=len(labels))\n",
    "\n",
    "        # Retain only selected labels\n",
    "        indices = np.searchsorted(sorted_labels, labels[:n_labels])\n",
    "        tp_sum = tp_sum[indices]\n",
    "        true_sum = true_sum[indices]\n",
    "        pred_sum = pred_sum[indices]\n",
    "\n",
    "    else:\n",
    "        sum_axis = 1 if samplewise else 0\n",
    "\n",
    "        # All labels are index integers for multilabel.\n",
    "        # Select labels:\n",
    "        if not np.array_equal(labels, present_labels):\n",
    "            if np.max(labels) > np.max(present_labels):\n",
    "                raise ValueError('All labels must be in [0, n labels) for '\n",
    "                                 'multilabel targets. '\n",
    "                                 'Got %d > %d' %\n",
    "                                 (np.max(labels), np.max(present_labels)))\n",
    "            if np.min(labels) < 0:\n",
    "                raise ValueError('All labels must be in [0, n labels) for '\n",
    "                                 'multilabel targets. '\n",
    "                                 'Got %d < 0' % np.min(labels))\n",
    "\n",
    "        if n_labels is not None:\n",
    "            y_true = y_true[:, labels[:n_labels]]\n",
    "            y_pred = y_pred[:, labels[:n_labels]]\n",
    "\n",
    "        # calculate weighted counts\n",
    "        true_and_pred = y_true.multiply(y_pred)\n",
    "        tp_sum = count_nonzero(true_and_pred, axis=sum_axis,\n",
    "                               sample_weight=sample_weight)\n",
    "        pred_sum = count_nonzero(y_pred, axis=sum_axis,\n",
    "                                 sample_weight=sample_weight)\n",
    "        true_sum = count_nonzero(y_true, axis=sum_axis,\n",
    "                                 sample_weight=sample_weight)\n",
    "\n",
    "    fp = pred_sum - tp_sum\n",
    "    fn = true_sum - tp_sum\n",
    "    tp = tp_sum\n",
    "\n",
    "    if sample_weight is not None and samplewise:\n",
    "        sample_weight = np.array(sample_weight)\n",
    "        tp = np.array(tp)\n",
    "        fp = np.array(fp)\n",
    "        fn = np.array(fn)\n",
    "        tn = sample_weight * y_true.shape[1] - tp - fp - fn\n",
    "    elif sample_weight is not None:\n",
    "        tn = sum(sample_weight) - tp - fp - fn\n",
    "    elif samplewise:\n",
    "        tn = y_true.shape[1] - tp - fp - fn\n",
    "    else:\n",
    "        tn = y_true.shape[0] - tp - fp - fn\n",
    "\n",
    "    return np.array([tn, fp, fn, tp]).T.reshape(-1, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerMixin:\n",
    "    \"\"\"Mixin class for all transformers in scikit-learn.\"\"\"\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        \"\"\"\n",
    "        Fit to data, then transform it.\n",
    "        Fits transformer to X and y with optional parameters fit_params\n",
    "        and returns a transformed version of X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array of shape [n_samples, n_features]\n",
    "            Training set.\n",
    "        y : numpy array of shape [n_samples]\n",
    "            Target values.\n",
    "        **fit_params : dict\n",
    "            Additional fit parameters.\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : numpy array of shape [n_samples, n_features_new]\n",
    "            Transformed array.\n",
    "        \"\"\"\n",
    "        # non-optimized default implementation; override when a better\n",
    "        # method is possible for a given clustering algorithm\n",
    "        if y is None:\n",
    "            # fit method of arity 1 (unsupervised transformation)\n",
    "            return self.fit(X, **fit_params).transform(X)\n",
    "        else:\n",
    "            # fit method of arity 2 (supervised transformation)\n",
    "            return self.fit(X, y, **fit_params).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEstimator:\n",
    "    \"\"\"Base class for all estimators in scikit-learn\n",
    "    Notes\n",
    "    -----\n",
    "    All estimators should specify all the parameters that can be set\n",
    "    at the class level in their ``__init__`` as explicit keyword\n",
    "    arguments (no ``*args`` or ``**kwargs``).\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def _get_param_names(cls):\n",
    "        \"\"\"Get parameter names for the estimator\"\"\"\n",
    "        # fetch the constructor or the original constructor before\n",
    "        # deprecation wrapping if any\n",
    "        init = getattr(cls.__init__, 'deprecated_original', cls.__init__)\n",
    "        if init is object.__init__:\n",
    "            # No explicit constructor to introspect\n",
    "            return []\n",
    "\n",
    "        # introspect the constructor arguments to find the model parameters\n",
    "        # to represent\n",
    "        init_signature = inspect.signature(init)\n",
    "        # Consider the constructor parameters excluding 'self'\n",
    "        parameters = [p for p in init_signature.parameters.values()\n",
    "                      if p.name != 'self' and p.kind != p.VAR_KEYWORD]\n",
    "        for p in parameters:\n",
    "            if p.kind == p.VAR_POSITIONAL:\n",
    "                raise RuntimeError(\"scikit-learn estimators should always \"\n",
    "                                   \"specify their parameters in the signature\"\n",
    "                                   \" of their __init__ (no varargs).\"\n",
    "                                   \" %s with constructor %s doesn't \"\n",
    "                                   \" follow this convention.\"\n",
    "                                   % (cls, init_signature))\n",
    "        # Extract and sort argument names excluding 'self'\n",
    "        return sorted([p.name for p in parameters])\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"\n",
    "        Get parameters for this estimator.\n",
    "        Parameters\n",
    "        ----------\n",
    "        deep : bool, default=True\n",
    "            If True, will return the parameters for this estimator and\n",
    "            contained subobjects that are estimators.\n",
    "        Returns\n",
    "        -------\n",
    "        params : mapping of string to any\n",
    "            Parameter names mapped to their values.\n",
    "        \"\"\"\n",
    "        out = dict()\n",
    "        for key in self._get_param_names():\n",
    "            try:\n",
    "                value = getattr(self, key)\n",
    "            except AttributeError:\n",
    "                warnings.warn('From version 0.24, get_params will raise an '\n",
    "                              'AttributeError if a parameter cannot be '\n",
    "                              'retrieved as an instance attribute. Previously '\n",
    "                              'it would return None.',\n",
    "                              FutureWarning)\n",
    "                value = None\n",
    "            if deep and hasattr(value, 'get_params'):\n",
    "                deep_items = value.get_params().items()\n",
    "                out.update((key + '__' + k, val) for k, val in deep_items)\n",
    "            out[key] = value\n",
    "        return out\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        \"\"\"\n",
    "        Set the parameters of this estimator.\n",
    "        The method works on simple estimators as well as on nested objects\n",
    "        (such as pipelines). The latter have parameters of the form\n",
    "        ``<component>__<parameter>`` so that it's possible to update each\n",
    "        component of a nested object.\n",
    "        Parameters\n",
    "        ----------\n",
    "        **params : dict\n",
    "            Estimator parameters.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Estimator instance.\n",
    "        \"\"\"\n",
    "        if not params:\n",
    "            # Simple optimization to gain speed (inspect is slow)\n",
    "            return self\n",
    "        valid_params = self.get_params(deep=True)\n",
    "\n",
    "        nested_params = defaultdict(dict)  # grouped by prefix\n",
    "        for key, value in params.items():\n",
    "            key, delim, sub_key = key.partition('__')\n",
    "            if key not in valid_params:\n",
    "                raise ValueError('Invalid parameter %s for estimator %s. '\n",
    "                                 'Check the list of available parameters '\n",
    "                                 'with `estimator.get_params().keys()`.' %\n",
    "                                 (key, self))\n",
    "\n",
    "            if delim:\n",
    "                nested_params[key][sub_key] = value\n",
    "            else:\n",
    "                setattr(self, key, value)\n",
    "                valid_params[key] = value\n",
    "\n",
    "        for key, sub_params in nested_params.items():\n",
    "            valid_params[key].set_params(**sub_params)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __repr__(self, N_CHAR_MAX=700):\n",
    "        # N_CHAR_MAX is the (approximate) maximum number of non-blank\n",
    "        # characters to render. We pass it as an optional parameter to ease\n",
    "        # the tests.\n",
    "\n",
    "        from .utils._pprint import _EstimatorPrettyPrinter\n",
    "\n",
    "        N_MAX_ELEMENTS_TO_SHOW = 30  # number of elements to show in sequences\n",
    "\n",
    "        # use ellipsis for sequences with a lot of elements\n",
    "        pp = _EstimatorPrettyPrinter(\n",
    "            compact=True, indent=1, indent_at_name=True,\n",
    "            n_max_elements_to_show=N_MAX_ELEMENTS_TO_SHOW)\n",
    "\n",
    "        repr_ = pp.pformat(self)\n",
    "\n",
    "        # Use bruteforce ellipsis when there are a lot of non-blank characters\n",
    "        n_nonblank = len(''.join(repr_.split()))\n",
    "        if n_nonblank > N_CHAR_MAX:\n",
    "            lim = N_CHAR_MAX // 2  # apprx number of chars to keep on both ends\n",
    "            regex = r'^(\\s*\\S){%d}' % lim\n",
    "            # The regex '^(\\s*\\S){%d}' % n\n",
    "            # matches from the start of the string until the nth non-blank\n",
    "            # character:\n",
    "            # - ^ matches the start of string\n",
    "            # - (pattern){n} matches n repetitions of pattern\n",
    "            # - \\s*\\S matches a non-blank char following zero or more blanks\n",
    "            left_lim = re.match(regex, repr_).end()\n",
    "            right_lim = re.match(regex, repr_[::-1]).end()\n",
    "\n",
    "            if '\\n' in repr_[left_lim:-right_lim]:\n",
    "                # The left side and right side aren't on the same line.\n",
    "                # To avoid weird cuts, e.g.:\n",
    "                # categoric...ore',\n",
    "                # we need to start the right side with an appropriate newline\n",
    "                # character so that it renders properly as:\n",
    "                # categoric...\n",
    "                # handle_unknown='ignore',\n",
    "                # so we add [^\\n]*\\n which matches until the next \\n\n",
    "                regex += r'[^\\n]*\\n'\n",
    "                right_lim = re.match(regex, repr_[::-1]).end()\n",
    "\n",
    "            ellipsis = '...'\n",
    "            if left_lim + len(ellipsis) < len(repr_) - right_lim:\n",
    "                # Only add ellipsis if it results in a shorter repr\n",
    "                repr_ = repr_[:left_lim] + '...' + repr_[-right_lim:]\n",
    "\n",
    "        return repr_\n",
    "\n",
    "    def __getstate__(self):\n",
    "        try:\n",
    "            state = super().__getstate__()\n",
    "        except AttributeError:\n",
    "            state = self.__dict__.copy()\n",
    "\n",
    "        if type(self).__module__.startswith('sklearn.'):\n",
    "            return dict(state.items(), _sklearn_version=__version__)\n",
    "        else:\n",
    "            return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if type(self).__module__.startswith('sklearn.'):\n",
    "            pickle_version = state.pop(\"_sklearn_version\", \"pre-0.18\")\n",
    "            if pickle_version != __version__:\n",
    "                warnings.warn(\n",
    "                    \"Trying to unpickle estimator {0} from version {1} when \"\n",
    "                    \"using version {2}. This might lead to breaking code or \"\n",
    "                    \"invalid results. Use at your own risk.\".format(\n",
    "                        self.__class__.__name__, pickle_version, __version__),\n",
    "                    UserWarning)\n",
    "        try:\n",
    "            super().__setstate__(state)\n",
    "        except AttributeError:\n",
    "            self.__dict__.update(state)\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return _DEFAULT_TAGS\n",
    "\n",
    "    def _get_tags(self):\n",
    "        collected_tags = {}\n",
    "        for base_class in reversed(inspect.getmro(self.__class__)):\n",
    "            if hasattr(base_class, '_more_tags'):\n",
    "                # need the if because mixins might not have _more_tags\n",
    "                # but might do redundant work in estimators\n",
    "                # (i.e. calling more tags on BaseEstimator multiple times)\n",
    "                more_tags = base_class._more_tags(self)\n",
    "                collected_tags.update(more_tags)\n",
    "        return collected_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"Encode target labels with value between 0 and n_classes-1.\n",
    "    This transformer should be used to encode target values, *i.e.* `y`, and\n",
    "    not the input `X`.\n",
    "    Read more in the :ref:`User Guide <preprocessing_targets>`.\n",
    "    .. versionadded:: 0.12\n",
    "    Attributes\n",
    "    ----------\n",
    "    classes_ : array of shape (n_class,)\n",
    "        Holds the label for each class.\n",
    "    Examples\n",
    "    --------\n",
    "    `LabelEncoder` can be used to normalize labels.\n",
    "    >>> from sklearn import preprocessing\n",
    "    >>> le = preprocessing.LabelEncoder()\n",
    "    >>> le.fit([1, 2, 2, 6])\n",
    "    LabelEncoder()\n",
    "    >>> le.classes_\n",
    "    array([1, 2, 6])\n",
    "    >>> le.transform([1, 1, 2, 6])\n",
    "    array([0, 0, 1, 2]...)\n",
    "    >>> le.inverse_transform([0, 0, 1, 2])\n",
    "    array([1, 1, 2, 6])\n",
    "    It can also be used to transform non-numerical labels (as long as they are\n",
    "    hashable and comparable) to numerical labels.\n",
    "    >>> le = preprocessing.LabelEncoder()\n",
    "    >>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\n",
    "    LabelEncoder()\n",
    "    >>> list(le.classes_)\n",
    "    ['amsterdam', 'paris', 'tokyo']\n",
    "    >>> le.transform([\"tokyo\", \"tokyo\", \"paris\"])\n",
    "    array([2, 2, 1]...)\n",
    "    >>> list(le.inverse_transform([2, 2, 1]))\n",
    "    ['tokyo', 'tokyo', 'paris']\n",
    "    See also\n",
    "    --------\n",
    "    sklearn.preprocessing.OrdinalEncoder : Encode categorical features\n",
    "        using an ordinal encoding scheme.\n",
    "    sklearn.preprocessing.OneHotEncoder : Encode categorical features\n",
    "        as a one-hot numeric array.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, y):\n",
    "        \"\"\"Fit label encoder\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values.\n",
    "        Returns\n",
    "        -------\n",
    "        self : returns an instance of self.\n",
    "        \"\"\"\n",
    "        y = column_or_1d(y, warn=True)\n",
    "        self.classes_ = _encode(y)\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, y):\n",
    "        \"\"\"Fit label encoder and return encoded labels\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : array-like of shape [n_samples]\n",
    "            Target values.\n",
    "        Returns\n",
    "        -------\n",
    "        y : array-like of shape [n_samples]\n",
    "        \"\"\"\n",
    "        y = column_or_1d(y, warn=True)\n",
    "        self.classes_, y = _encode(y, encode=True)\n",
    "        return y\n",
    "\n",
    "    def transform(self, y):\n",
    "        \"\"\"Transform labels to normalized encoding.\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : array-like of shape [n_samples]\n",
    "            Target values.\n",
    "        Returns\n",
    "        -------\n",
    "        y : array-like of shape [n_samples]\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        y = column_or_1d(y, warn=True)\n",
    "        # transform of empty array is empty array\n",
    "        if _num_samples(y) == 0:\n",
    "            return np.array([])\n",
    "\n",
    "        _, y = _encode(y, uniques=self.classes_, encode=True)\n",
    "        return y\n",
    "\n",
    "    def inverse_transform(self, y):\n",
    "        \"\"\"Transform labels back to original encoding.\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : numpy array of shape [n_samples]\n",
    "            Target values.\n",
    "        Returns\n",
    "        -------\n",
    "        y : numpy array of shape [n_samples]\n",
    "        \"\"\"\n",
    "        check_is_fitted(self)\n",
    "        y = column_or_1d(y, warn=True)\n",
    "        # inverse transform of empty array is empty array\n",
    "        if _num_samples(y) == 0:\n",
    "            return np.array([])\n",
    "\n",
    "        diff = np.setdiff1d(y, np.arange(len(self.classes_)))\n",
    "        if len(diff):\n",
    "            raise ValueError(\n",
    "                    \"y contains previously unseen labels: %s\" % str(diff))\n",
    "        y = np.asarray(y)\n",
    "        return self.classes_[y]\n",
    "\n",
    "    def _more_tags(self):\n",
    "        return {'X_types': ['1dlabels']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encode(values, uniques=None, encode=False, check_unknown=True):\n",
    "    \"\"\"Helper function to factorize (find uniques) and encode values.\n",
    "    Uses pure python method for object dtype, and numpy method for\n",
    "    all other dtypes.\n",
    "    The numpy method has the limitation that the `uniques` need to\n",
    "    be sorted. Importantly, this is not checked but assumed to already be\n",
    "    the case. The calling method needs to ensure this for all non-object\n",
    "    values.\n",
    "    Parameters\n",
    "    ----------\n",
    "    values : array\n",
    "        Values to factorize or encode.\n",
    "    uniques : array, optional\n",
    "        If passed, uniques are not determined from passed values (this\n",
    "        can be because the user specified categories, or because they\n",
    "        already have been determined in fit).\n",
    "    encode : bool, default False\n",
    "        If True, also encode the values into integer codes based on `uniques`.\n",
    "    check_unknown : bool, default True\n",
    "        If True, check for values in ``values`` that are not in ``unique``\n",
    "        and raise an error. This is ignored for object dtype, and treated as\n",
    "        True in this case. This parameter is useful for\n",
    "        _BaseEncoder._transform() to avoid calling _encode_check_unknown()\n",
    "        twice.\n",
    "    Returns\n",
    "    -------\n",
    "    uniques\n",
    "        If ``encode=False``. The unique values are sorted if the `uniques`\n",
    "        parameter was None (and thus inferred from the data).\n",
    "    (uniques, encoded)\n",
    "        If ``encode=True``.\n",
    "    \"\"\"\n",
    "    if values.dtype == object:\n",
    "        try:\n",
    "            res = _encode_python(values, uniques, encode)\n",
    "        except TypeError:\n",
    "            raise TypeError(\"argument must be a string or number\")\n",
    "        return res\n",
    "    else:\n",
    "        return _encode_numpy(values, uniques, encode,\n",
    "                             check_unknown=check_unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encode_numpy(values, uniques=None, encode=False, check_unknown=True):\n",
    "    # only used in _encode below, see docstring there for details\n",
    "    if uniques is None:\n",
    "        if encode:\n",
    "            uniques, encoded = np.unique(values, return_inverse=True)\n",
    "            return uniques, encoded\n",
    "        else:\n",
    "            # unique sorts\n",
    "            return np.unique(values)\n",
    "    if encode:\n",
    "        if check_unknown:\n",
    "            diff = _encode_check_unknown(values, uniques)\n",
    "            if diff:\n",
    "                raise ValueError(\"y contains previously unseen labels: %s\"\n",
    "                                 % str(diff))\n",
    "        encoded = np.searchsorted(uniques, values)\n",
    "        return uniques, encoded\n",
    "    else:\n",
    "        return uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_is_fitted(estimator, attributes='deprecated', msg=None,\n",
    "                    all_or_any='deprecated'):\n",
    "    \"\"\"Perform is_fitted validation for estimator.\n",
    "    Checks if the estimator is fitted by verifying the presence of\n",
    "    fitted attributes (ending with a trailing underscore) and otherwise\n",
    "    raises a NotFittedError with the given message.\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator instance.\n",
    "        estimator instance for which the check is performed.\n",
    "    attributes : deprecated, ignored\n",
    "        .. deprecated:: 0.22\n",
    "           `attributes` is deprecated, is currently ignored and will be removed\n",
    "           in 0.23.\n",
    "    msg : string\n",
    "        The default error message is, \"This %(name)s instance is not fitted\n",
    "        yet. Call 'fit' with appropriate arguments before using this\n",
    "        estimator.\"\n",
    "        For custom messages if \"%(name)s\" is present in the message string,\n",
    "        it is substituted for the estimator name.\n",
    "        Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\".\n",
    "    all_or_any : deprecated, ignored\n",
    "        .. deprecated:: 0.21\n",
    "           `all_or_any` is deprecated, is currently ignored and will be removed\n",
    "           in 0.23.\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    Raises\n",
    "    ------\n",
    "    NotFittedError\n",
    "        If the attributes are not found.\n",
    "    \"\"\"\n",
    "    if attributes != 'deprecated':\n",
    "        warnings.warn(\"Passing attributes to check_is_fitted is deprecated\"\n",
    "                      \" and will be removed in 0.23. The attributes \"\n",
    "                      \"argument is ignored.\", FutureWarning)\n",
    "    if all_or_any != 'deprecated':\n",
    "        warnings.warn(\"Passing all_or_any to check_is_fitted is deprecated\"\n",
    "                      \" and will be removed in 0.23. The any_or_all \"\n",
    "                      \"argument is ignored.\", FutureWarning)\n",
    "    if isclass(estimator):\n",
    "        raise TypeError(\"{} is a class, not an instance.\".format(estimator))\n",
    "    if msg is None:\n",
    "        msg = (\"This %(name)s instance is not fitted yet. Call 'fit' with \"\n",
    "               \"appropriate arguments before using this estimator.\")\n",
    "\n",
    "    if not hasattr(estimator, 'fit'):\n",
    "        raise TypeError(\"%s is not an estimator instance.\" % (estimator))\n",
    "\n",
    "    attrs = [v for v in vars(estimator)\n",
    "             if (v.endswith(\"_\") or v.startswith(\"_\"))\n",
    "             and not v.startswith(\"__\")]\n",
    "\n",
    "    if not attrs:\n",
    "        raise NotFittedError(msg % {'name': type(estimator).__name__})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encode_check_unknown(values, uniques, return_mask=False):\n",
    "    \"\"\"\n",
    "    Helper function to check for unknowns in values to be encoded.\n",
    "    Uses pure python method for object dtype, and numpy method for\n",
    "    all other dtypes.\n",
    "    Parameters\n",
    "    ----------\n",
    "    values : array\n",
    "        Values to check for unknowns.\n",
    "    uniques : array\n",
    "        Allowed uniques values.\n",
    "    return_mask : bool, default False\n",
    "        If True, return a mask of the same shape as `values` indicating\n",
    "        the valid values.\n",
    "    Returns\n",
    "    -------\n",
    "    diff : list\n",
    "        The unique values present in `values` and not in `uniques` (the\n",
    "        unknown values).\n",
    "    valid_mask : boolean array\n",
    "        Additionally returned if ``return_mask=True``.\n",
    "    \"\"\"\n",
    "    if values.dtype == object:\n",
    "        uniques_set = set(uniques)\n",
    "        diff = list(set(values) - uniques_set)\n",
    "        if return_mask:\n",
    "            if diff:\n",
    "                valid_mask = np.array([val in uniques_set for val in values])\n",
    "            else:\n",
    "                valid_mask = np.ones(len(values), dtype=bool)\n",
    "            return diff, valid_mask\n",
    "        else:\n",
    "            return diff\n",
    "    else:\n",
    "        unique_values = np.unique(values)\n",
    "        diff = list(np.setdiff1d(unique_values, uniques, assume_unique=True))\n",
    "        if return_mask:\n",
    "            if diff:\n",
    "                valid_mask = np.in1d(values, uniques)\n",
    "            else:\n",
    "                valid_mask = np.ones(len(values), dtype=bool)\n",
    "            return diff, valid_mask\n",
    "        else:\n",
    "            return diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC 与 AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC，Receiver Operating Characteristic。\n",
    "\n",
    "纵轴是真正例率 TPR，横轴是假正例率 FPR，也是基于混淆矩阵。\n",
    "\n",
    "$$\n",
    "TPR = \\frac {TP}{TP+FN}\n",
    "$$\n",
    "\n",
    "实际上就是 Recall\n",
    "\n",
    "$$\n",
    "FPR = \\frac {FP}{FP+TN}\n",
    "$$\n",
    "\n",
    "AUC，Area Under ROC Curve。也就是 ROC 曲线下的面积，面积越大一般认为模型效果越好。\n",
    "\n",
    "为什么？\n",
    "\n",
    "因为假正例率很低，真正例率很高时意味着模型 Recall 很好，同时误判的比例很小。\n",
    "\n",
    "对角线则对应着随机模型（各 50%），（0，1）点对应的是理想模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 核心"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC 与 PR 其实很类似，尤其 TPR 其实就是 Recall，也就是 **PR 曲线的横轴其实就是 ROC 曲线的纵轴**。\n",
    "即真正的正例中，有多少是被模型预测为正例。\n",
    "\n",
    "P 是模型的精准率，也就是预测是正例中，真正例的比例；\n",
    "FPR 是真正的反例中，被预测为正例的比例。\n",
    "\n",
    "换句话说：\n",
    "\n",
    "- PR 关注的是真实的正例/预测为正例中（分别对应 Recall 和 Precision），实际是正例的比例\n",
    "- ROC 关注的是真实的正例/反例中（分别对应 TPR 和 FPR），被预测为正例的比例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sklean 源码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "y = np.array([1, 1, 2, 2])\n",
    "pred = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_true = np.array([0, 0, 1, 1])\n",
    "y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "roc_auc_score(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "246.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
