{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "import warnings\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, log_loss\n",
    "from optbinning import OptimalBinning\n",
    "from typing import List, Optional, Union\n",
    "import pnlp\n",
    "\n",
    "import category_encoders as ce\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上一版的基础上，先固定模型不变，然后对特征工程做以下调整处理：\n",
    "\n",
    "- 删除缺失值：由于测试集也有缺失值，因此这个调整不做。\n",
    "- 类别（地区编码等）人工分桶：先不处理。\n",
    "- 删除异常值：先不处理。\n",
    "\n",
    "---\n",
    "\n",
    "- 数值不分桶：前后对比 0.7191 VS 0.7208(v2.0)\n",
    "- n 系列的整数都按类别分桶：前后对比 0.7208 VS 0.7203(v3.0)\n",
    "- 地区数据不分桶：前后对比 0.7203 VS 0.7232(v4.0)\n",
    "- 数值特征按 unique 值是否大于 50 来区分：前后对比 0.7232 VS 0.xxxx(v5.0)\n",
    "\n",
    "---\n",
    "\n",
    "- 数值特征不分桶，n系列按类别分桶，地区不分桶：v6.0\n",
    "- 数据特征不分桶，n系列作为数据特征，地区不分桶：v7.0\n",
    "\n",
    "---\n",
    "\n",
    "- 年龄 < 1 从 0.5 调整为 0，对象 NA 从 \"\" 替换为 NA：v8.0\n",
    "- 年龄 < 1 保持 0.5，NA 从 ”“ 替换为 NA：v9.0\n",
    "\n",
    "---\n",
    "\n",
    "回到 7.0\n",
    "\n",
    "- 增加 title：v10.0\n",
    "- 需要分桶的类别数据转换：v11.0\n",
    "- grade 和 subGrade 增加到类别里：v12.0\n",
    "- 去掉 grade 和 subGrade 的数值类别：v13.0\n",
    "- 增加 obj_need_bucket_feas 分桶，即 分桶+转换：v14.0\n",
    "\n",
    "---\n",
    "\n",
    "回到 7.0\n",
    "\n",
    "- v15.0\n",
    "\n",
    "\n",
    "特征交叉和特征选择也先不做。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v5.0 lgb_score_mean: 0.7279104550696077\n",
    "# v6.0 lgb_score_mean: 0.7292114750286548\n",
    "# v7.0 lgb_score_mean: 0.7302781867038274\n",
    "# v8.0 lgb_score_mean: 0.7298601320998748\n",
    "# v9.0 lgb_score_mean: 0.729909166985008\n",
    "# v10.0 lgb_score_mean: 0.7304943179820246\n",
    "# v11.0 lgb_score_mean: 0.7311966915796223\n",
    "# v12.0 lgb_score_mean: 0.7312080192382515\n",
    "# v13.0 lgb_score_mean: 0.7309075610525948\n",
    "# v14.0 lgb_score_mean: 0.7312571984016215\n",
    "# v15.0 lgb_score_mean: 0.7302878558046096\n",
    "\n",
    "# baseline: lgb_score_mean: 0.7312705586323757"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"/Users/HaoShaochun/Yam/FinancialRiskControl/data/train.csv\"\n",
    "test_file = \"/Users/HaoShaochun/Yam/FinancialRiskControl/data/testA.csv\"\n",
    "\n",
    "data_train = pd.read_csv(train_file)\n",
    "data_test = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_one_feas = [\n",
    "    \"initialListStatus\", \"applicationType\", \n",
    "]\n",
    "\n",
    "num_not_bucket_feas = [\n",
    "    \"annualIncome\",\n",
    "    \"term\", \"employmentLength\", \n",
    "    \"loanAmnt\", \"interestRate\", \"installment\", \"dti\", \n",
    "    \"delinquency_2years\",\n",
    "    \"ficoRangeLow\", \"ficoRangeHigh\", \n",
    "    \"openAcc\", \n",
    "    \"pubRec\", \"pubRecBankruptcies\",\n",
    "    \"revolBal\", \"revolUtil\", \"totalAcc\",\n",
    "    \"earliesCreditLine\", \n",
    "    \"grade\", \"subGrade\"\n",
    "]\n",
    "\n",
    "num_need_bucket_feas = [\n",
    "]\n",
    "\n",
    "obj_not_bucket_feas = [\n",
    "    \"homeOwnership\", \"verificationStatus\", \"n11\", \"n12\",\n",
    "]\n",
    "\n",
    "obj_need_bucket_feas = [\n",
    "    \"employmentTitle\", \"postCode\", \"title\",\n",
    "    \"n0\", \"n1\", \"n2\", \n",
    "    'n4', 'n5', 'n6', 'n7', 'n8', 'n9', 'n10', 'n13', 'n14',\n",
    "    \"regionCode\", \"purpose\"\n",
    "]\n",
    "\n",
    "na_ave_feas = [\"dti\", \"revolUtil\"]\n",
    "na_ave_int_feas = [\"employmentLength\", \"pubRecBankruptcies\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_float(x):\n",
    "    return bool(int(x) - x)\n",
    "\n",
    "def all_data_is_float(df: pd.DataFrame, feature: str):\n",
    "    uniq = df[feature].unique()\n",
    "    for i in uniq:\n",
    "        if pd.isna(i):\n",
    "            continue\n",
    "        if is_float(i):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def drop_given_features(df: pd.DataFrame, feature_list: List[str]) -> pd.DataFrame:\n",
    "    return df.drop(columns=feature_list)\n",
    "\n",
    "def drop_uniquevalue_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    need_drop_feas = [col for col in df.columns if df[col].nunique() <= 1]\n",
    "    return df.drop(columns=need_drop_feas)\n",
    "\n",
    "def convert_num_to_obj(x: Union[int, float]):\n",
    "    if pd.isna(x):\n",
    "        return x\n",
    "    else:\n",
    "        return str(int(x))\n",
    "\n",
    "def convert_float_to_int(x: float):\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return int(x)\n",
    "\n",
    "def deal_employmentLength(x):\n",
    "    if pd.notna(x):\n",
    "        if x == \"10+ years\":\n",
    "            return 10\n",
    "        elif x == \"< 1 year\":\n",
    "            return 0\n",
    "        else:\n",
    "            return int(x[0])\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "insignificant_feas = [\"issueDate\"]\n",
    "grade_dct = dict(zip(['A', 'B', 'C', 'D', 'E', 'F', 'G'], range(10, 80, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(\n",
    "    data: pd.DataFrame, \n",
    "    num_optbs: list = [], \n",
    "    obj_optbs: list = [], \n",
    "    training_data: bool = True):\n",
    "    \n",
    "    if training_data:\n",
    "        num_optbs = []\n",
    "        obj_optbs = []\n",
    "    \n",
    "    data = drop_uniquevalue_features(data)\n",
    "    data = drop_given_features(data, insignificant_feas)\n",
    "\n",
    "    data['grade'] = data['grade'].map(grade_dct)\n",
    "    data[\"subGrade\"] = data[\"subGrade\"].apply(lambda x: grade_dct.get(x[0]) + int(x[1]))\n",
    "    data[\"employmentLength\"] = data[\"employmentLength\"].apply(deal_employmentLength)\n",
    "    data[\"earliesCreditLine\"] = data[\"earliesCreditLine\"].apply(lambda x: int(x[-4:]))\n",
    "    \n",
    "    # 转换\n",
    "    for i, fea in enumerate(obj_need_bucket_feas):\n",
    "        data[fea+'_cnts'] = data.groupby([fea])['id'].transform('count')\n",
    "        data[fea+'_rank'] = data.groupby([fea])['id'].rank(ascending=False).astype(int)\n",
    "        data = data.drop(columns=[fea])\n",
    "    \n",
    "    # 分桶：数值特征\n",
    "#     for i, fea in enumerate(num_need_bucket_feas):\n",
    "#         if training_data:\n",
    "#             optb = OptimalBinning(name=fea, dtype=\"numerical\", solver=\"cp\")\n",
    "#             optb.fit(data[fea], data[\"isDefault\"])\n",
    "#             num_optbs.append(optb)\n",
    "#         else:\n",
    "#             optb = num_optbs[i]\n",
    "#         data[fea] = optb.transform(data[fea])\n",
    "\n",
    "    # 分桶：类别特征\n",
    "#     for i, fea in enumerate(obj_need_bucket_feas):\n",
    "#         if fea.endswith(\"cnts\"):\n",
    "#             fea = fea + \"_cnts\"\n",
    "#         elif fea.endswith(\"rank\"):\n",
    "#             fea = fea + \"_rank\"\n",
    "            \n",
    "#         if training_data:\n",
    "#             try:\n",
    "#                 optb = OptimalBinning(name=fea, dtype=\"categorical\", solver=\"mip\", cat_cutoff=0.1)\n",
    "#                 optb.fit(data[fea], data[\"isDefault\"])\n",
    "#                 obj_optbs.append(optb)\n",
    "#             except Exception as err:\n",
    "#                 print(fea, data[fea].nunique())\n",
    "#                 continue\n",
    "#         else:\n",
    "#             optb = obj_optbs[i]\n",
    "#         data[fea] = optb.transform(data[fea])\n",
    "\n",
    "    data[na_ave_feas] = data[na_ave_feas].fillna(data[na_ave_feas].mean())\n",
    "    data[na_ave_int_feas] = data[na_ave_int_feas].fillna(data[na_ave_int_feas].mean().apply(int))\n",
    "    for fea in obj_need_bucket_feas:\n",
    "        fea1 = fea + \"_cnts\"\n",
    "        fea2 = fea + \"_rank\"\n",
    "        data[fea1] = data[fea1].fillna(data[fea1].mean())\n",
    "        data[fea2] = data[fea1].fillna(data[fea2].mean())\n",
    "    \n",
    "    data = pd.get_dummies(data, columns=obj_not_bucket_feas, drop_first=True)\n",
    "    \n",
    "    if training_data:\n",
    "        return data, num_optbs, obj_optbs\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, num_optbs, obj_optbs = process_data(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = process_data(data_test, num_optbs, obj_optbs, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(set(data_train.columns) & set(data_test.columns))\n",
    "features = [fea for fea in features if fea not in [\"isDefault\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data_train[features]\n",
    "x_test = data_test[features]\n",
    "y_train = data_train['isDefault']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_model(clf, train_x, train_y, test_x, clf_name):\n",
    "    folds = 5\n",
    "    seed = 2020\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "\n",
    "    train = np.zeros(train_x.shape[0])\n",
    "    test = np.zeros(test_x.shape[0])\n",
    "\n",
    "    cv_scores = []\n",
    "\n",
    "    for i, (train_index, valid_index) in enumerate(kf.split(train_x, train_y)):\n",
    "        print('************************************ {} ************************************'.format(str(i+1)))\n",
    "        trn_x, trn_y, val_x, val_y = train_x.iloc[train_index], train_y[train_index], train_x.iloc[valid_index], train_y[valid_index]\n",
    "\n",
    "        if clf_name == \"lgb\":\n",
    "            train_matrix = clf.Dataset(trn_x, label=trn_y)\n",
    "            valid_matrix = clf.Dataset(val_x, label=val_y)\n",
    "\n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'metric': 'auc',\n",
    "                'min_child_weight': 5,\n",
    "                'num_leaves': 2 ** 5,\n",
    "                'lambda_l2': 10,\n",
    "                'feature_fraction': 0.8,\n",
    "                'bagging_fraction': 0.8,\n",
    "                'bagging_freq': 4,\n",
    "                'learning_rate': 0.1,\n",
    "                'seed': 2020,\n",
    "                'nthread': 28,\n",
    "                'n_jobs':24,\n",
    "                'silent': True,\n",
    "                'verbose': -1,\n",
    "            }\n",
    "\n",
    "            model = clf.train(params, train_matrix, 50000, valid_sets=[train_matrix, valid_matrix], verbose_eval=200,early_stopping_rounds=200)\n",
    "            val_pred = model.predict(val_x, num_iteration=model.best_iteration)\n",
    "            test_pred = model.predict(test_x, num_iteration=model.best_iteration)\n",
    "            \n",
    "            # print(list(sorted(zip(features, model.feature_importance(\"gain\")), key=lambda x: x[1], reverse=True))[:20])\n",
    "                \n",
    "        if clf_name == \"xgb\":\n",
    "            train_matrix = clf.DMatrix(trn_x , label=trn_y)\n",
    "            valid_matrix = clf.DMatrix(val_x , label=val_y)\n",
    "            \n",
    "            params = {'booster': 'gbtree',\n",
    "                      'objective': 'binary:logistic',\n",
    "                      'eval_metric': 'auc',\n",
    "                      'gamma': 1,\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'max_depth': 5,\n",
    "                      'lambda': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'eta': 0.04,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2020,\n",
    "                      'nthread': 36,\n",
    "                      \"silent\": True,\n",
    "                      }\n",
    "            \n",
    "            watchlist = [(train_matrix, 'train'),(valid_matrix, 'eval')]\n",
    "            \n",
    "            model = clf.train(params, train_matrix, num_boost_round=50000, evals=watchlist, verbose_eval=200, early_stopping_rounds=200)\n",
    "            val_pred  = model.predict(valid_matrix, ntree_limit=model.best_ntree_limit)\n",
    "            test_pred = model.predict(test_x , ntree_limit=model.best_ntree_limit)\n",
    "                 \n",
    "        if clf_name == \"cat\":\n",
    "            params = {'learning_rate': 0.05, 'depth': 5, 'l2_leaf_reg': 10, 'bootstrap_type': 'Bernoulli',\n",
    "                      'od_type': 'Iter', 'od_wait': 50, 'random_seed': 11, 'allow_writing_files': False}\n",
    "            \n",
    "            model = clf(iterations=20000, **params)\n",
    "            model.fit(trn_x, trn_y, eval_set=(val_x, val_y),\n",
    "                      cat_features=[], use_best_model=True, verbose=500)\n",
    "            \n",
    "            val_pred  = model.predict(val_x)\n",
    "            test_pred = model.predict(test_x)\n",
    "            \n",
    "        train[valid_index] = val_pred\n",
    "        test = test_pred / kf.n_splits\n",
    "        cv_scores.append(roc_auc_score(val_y, val_pred))\n",
    "        \n",
    "        print(cv_scores)\n",
    "        \n",
    "    print(\"%s_scotrainre_list:\" % clf_name, cv_scores)\n",
    "    print(\"%s_score_mean:\" % clf_name, np.mean(cv_scores))\n",
    "    print(\"%s_score_std:\" % clf_name, np.std(cv_scores))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_model(x_train, y_train, x_test):\n",
    "    lgb_train, lgb_test = cv_model(lgb, x_train, y_train, x_test, \"lgb\")\n",
    "    return lgb_train, lgb_test\n",
    "\n",
    "def xgb_model(x_train, y_train, x_test):\n",
    "    xgb_train, xgb_test = cv_model(xgb, x_train, y_train, x_test, \"xgb\")\n",
    "    return xgb_train, xgb_test\n",
    "\n",
    "def cat_model(x_train, y_train, x_test):\n",
    "    cat_train, cat_test = cv_model(CatBoostRegressor, x_train, y_train, x_test, \"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************ 1 ************************************\n",
      "[LightGBM] [Warning] num_threads is set with n_jobs=24, nthread=28 will be ignored. Current value: num_threads=24\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's auc: 0.742068\tvalid_1's auc: 0.72961\n",
      "[400]\ttraining's auc: 0.75501\tvalid_1's auc: 0.730788\n",
      "[600]\ttraining's auc: 0.766032\tvalid_1's auc: 0.730912\n",
      "[800]\ttraining's auc: 0.776082\tvalid_1's auc: 0.730865\n",
      "Early stopping, best iteration is:\n",
      "[654]\ttraining's auc: 0.768859\tvalid_1's auc: 0.731115\n",
      "[0.7311148422768661]\n",
      "************************************ 2 ************************************\n",
      "[LightGBM] [Warning] num_threads is set with n_jobs=24, nthread=28 will be ignored. Current value: num_threads=24\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's auc: 0.743075\tvalid_1's auc: 0.725659\n",
      "[400]\ttraining's auc: 0.755906\tvalid_1's auc: 0.726767\n",
      "[600]\ttraining's auc: 0.766757\tvalid_1's auc: 0.727227\n",
      "[800]\ttraining's auc: 0.77723\tvalid_1's auc: 0.727332\n",
      "Early stopping, best iteration is:\n",
      "[710]\ttraining's auc: 0.772607\tvalid_1's auc: 0.727422\n",
      "[0.7311148422768661, 0.727422124643655]\n",
      "************************************ 3 ************************************\n",
      "[LightGBM] [Warning] num_threads is set with n_jobs=24, nthread=28 will be ignored. Current value: num_threads=24\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's auc: 0.741881\tvalid_1's auc: 0.730163\n",
      "[400]\ttraining's auc: 0.75455\tvalid_1's auc: 0.731362\n",
      "[600]\ttraining's auc: 0.765952\tvalid_1's auc: 0.731753\n",
      "Early stopping, best iteration is:\n",
      "[524]\ttraining's auc: 0.761919\tvalid_1's auc: 0.731801\n",
      "[0.7311148422768661, 0.727422124643655, 0.7318008859742777]\n",
      "************************************ 4 ************************************\n",
      "[LightGBM] [Warning] num_threads is set with n_jobs=24, nthread=28 will be ignored. Current value: num_threads=24\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's auc: 0.742291\tvalid_1's auc: 0.729135\n",
      "[400]\ttraining's auc: 0.754829\tvalid_1's auc: 0.730007\n",
      "[600]\ttraining's auc: 0.766256\tvalid_1's auc: 0.730673\n",
      "[800]\ttraining's auc: 0.776744\tvalid_1's auc: 0.730657\n",
      "Early stopping, best iteration is:\n",
      "[623]\ttraining's auc: 0.767645\tvalid_1's auc: 0.730747\n",
      "[0.7311148422768661, 0.727422124643655, 0.7318008859742777, 0.7307473514063647]\n",
      "************************************ 5 ************************************\n",
      "[LightGBM] [Warning] num_threads is set with n_jobs=24, nthread=28 will be ignored. Current value: num_threads=24\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[200]\ttraining's auc: 0.742671\tvalid_1's auc: 0.728153\n",
      "[400]\ttraining's auc: 0.755277\tvalid_1's auc: 0.729347\n",
      "[600]\ttraining's auc: 0.766272\tvalid_1's auc: 0.729526\n",
      "Early stopping, best iteration is:\n",
      "[535]\ttraining's auc: 0.762675\tvalid_1's auc: 0.729568\n",
      "[0.7311148422768661, 0.727422124643655, 0.7318008859742777, 0.7307473514063647, 0.7295684174238821]\n",
      "lgb_scotrainre_list: [0.7311148422768661, 0.727422124643655, 0.7318008859742777, 0.7307473514063647, 0.7295684174238821]\n",
      "lgb_score_mean: 0.730130724345009\n",
      "lgb_score_std: 0.001535633961606915\n"
     ]
    }
   ],
   "source": [
    "lgb_train, lgb_test = lgb_model(x_train, y_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out_file = \"/Users/HaoShaochun/Yam/FinancialRiskControl/data/testA_result_V15.0.csv\"\n",
    "\n",
    "rh_test = lgb_test#*0.5 + xgb_test*0.5\n",
    "data_test['isDefault'] = rh_test\n",
    "data_test[['id','isDefault']].to_csv(test_out_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
